{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis using LSTM\n",
        "\n",
        "Medha Sreenivasan-ms1112\n",
        "\n",
        "Thwisha Nahender-tn130"
      ],
      "metadata": {
        "id": "6BFrXTTLev_b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gH2LGlZn9t2U"
      },
      "outputs": [],
      "source": [
        "#Importing the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib as pyplot\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from string import punctuation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/googledrive\",force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w48A6J3P94wL",
        "outputId": "86923476-8372-400e-a1b0-94fcb62f4f54"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/googledrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMDB Dataset**"
      ],
      "metadata": {
        "id": "duqqmLgYY1Cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv(\"/content/googledrive/MyDrive/NLP Project/IMDB Dataset.csv\")"
      ],
      "metadata": {
        "id": "0oyI6FdX9zZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Icz1Q-QA-esX",
        "outputId": "fd12eda3-8929-44f2-c513-27d2c578a33d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  One of the other reviewers has mentioned that ...  positive\n",
              "1  A wonderful little production. <br /><br />The...  positive\n",
              "2  I thought this was a wonderful way to spend ti...  positive\n",
              "3  Basically there's a family where a little boy ...  negative\n",
              "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "5  Probably my all-time favorite movie, a story o...  positive\n",
              "6  I sure would like to see a resurrection of a u...  positive\n",
              "7  This show was an amazing, fresh & innovative i...  negative\n",
              "8  Encouraged by the positive comments about this...  negative\n",
              "9  If you like original gut wrenching laughter yo...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6ddf4a1c-9933-4fb2-a01d-e78bd5de9096\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Probably my all-time favorite movie, a story o...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>If you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6ddf4a1c-9933-4fb2-a01d-e78bd5de9096')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6ddf4a1c-9933-4fb2-a01d-e78bd5de9096 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6ddf4a1c-9933-4fb2-a01d-e78bd5de9096');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9fc2e6d3-3184-4c99-ad63-9934cf4fd6fb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9fc2e6d3-3184-4c99-ad63-9934cf4fd6fb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9fc2e6d3-3184-4c99-ad63-9934cf4fd6fb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of the dataset:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiC4n6fvq1E0",
        "outputId": "1c1ec4f8-3a12-4e9a-d1b6-d89f3af57a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the dataset: (50000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "T-5amDaAsWlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(punctuation)\n",
        "\n",
        "\n",
        "df['review'] = df['review'].str.lower()\n",
        "df['review'] = df['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
        "\n",
        "# Split by new lines and spaces\n",
        "df['review'] = df['review'].str.replace('\\n', ' ')\n",
        "df['review'] =df['review'].str.split()\n",
        "\n",
        "# Create a list of words\n",
        "df['review'] = df['review'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HfBCCKLP3ay",
        "outputId": "99a1b87d-6e33-46f6-c372-229a8a6bf47d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataframe after preprocessing\n",
        "df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "6ybDWsa4j8Zy",
        "outputId": "02aed61e-10de-4f5b-e210-acf5a4a3d76c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review sentiment\n",
              "0  one of the other reviewers has mentioned that ...  positive\n",
              "1  a wonderful little production br br the filmin...  positive\n",
              "2  i thought this was a wonderful way to spend ti...  positive\n",
              "3  basically theres a family where a little boy j...  negative\n",
              "4  petter matteis love in the time of money is a ...  positive\n",
              "5  probably my alltime favorite movie a story of ...  positive\n",
              "6  i sure would like to see a resurrection of a u...  positive\n",
              "7  this show was an amazing fresh innovative idea...  negative\n",
              "8  encouraged by the positive comments about this...  negative\n",
              "9  if you like original gut wrenching laughter yo...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13513d9b-ed2b-4469-9755-01b6ed4ef2d3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>one of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a wonderful little production br br the filmin...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>i thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>basically theres a family where a little boy j...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>petter matteis love in the time of money is a ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>probably my alltime favorite movie a story of ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>i sure would like to see a resurrection of a u...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>this show was an amazing fresh innovative idea...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>encouraged by the positive comments about this...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>if you like original gut wrenching laughter yo...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13513d9b-ed2b-4469-9755-01b6ed4ef2d3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13513d9b-ed2b-4469-9755-01b6ed4ef2d3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13513d9b-ed2b-4469-9755-01b6ed4ef2d3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-5809f681-a801-406c-911a-6f1e2b8c8883\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5809f681-a801-406c-911a-6f1e2b8c8883')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-5809f681-a801-406c-911a-6f1e2b8c8883 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_split = df['review'].tolist()\n",
        "labels_split = df['sentiment'].tolist()"
      ],
      "metadata": {
        "id": "m7KmgnNzks9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "v4JJ6n4qsc03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['review'].values\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(reviews)\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(reviews)\n",
        "\n",
        "\n",
        "word_to_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "print(\"Dictionary: \", dict(list(word_to_index.items())[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsMCf63-hBgx",
        "outputId": "430f8cb6-1e43-49e6-9d41-09dbf005047c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary:  {'the': 1, 'and': 2, 'a': 3, 'of': 4, 'to': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_reviews(reviews_split, word_to_index):\n",
        "\n",
        "\n",
        "    reviews_ints = []\n",
        "    for review in reviews_split:\n",
        "      reviews_ints.append([word_to_index[word] for word in review.split()])\n",
        "\n",
        "    return reviews_ints"
      ],
      "metadata": {
        "id": "LiGPKwDIQCMp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ints = tokenize_reviews(reviews_split, word_to_index)"
      ],
      "metadata": {
        "id": "cc0hHx5uk3Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Unique words: ', len((word_to_index)))\n",
        "print()\n",
        "\n",
        "print('Tokenized review: \\n', reviews_ints[:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xclJhIPpl8Cy",
        "outputId": "b5548bb7-4b17-4f38-b73e-ddd5a565de54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words:  181685\n",
            "\n",
            "Tokenized review: \n",
            " [[28, 4, 1, 77, 1941, 44, 1063, 11, 100, 145, 40, 479, 3324, 393, 461, 26, 3190, 34, 23, 205, 14, 10, 6, 601, 48, 590, 15, 2137, 12, 1, 87, 146, 11, 3255, 69, 42, 3324, 13, 29, 5600, 2, 15378, 134, 4, 582, 61, 282, 7, 205, 35, 1, 670, 138, 1707, 69, 10, 6, 21, 3, 118, 16, 1, 8330, 5794, 39, 11861, 10, 118, 2508, 55, 6064, 15, 5636, 5, 1470, 381, 39, 582, 29, 6, 3407, 7, 1, 352, 340, 4, 1, 23503, 12, 8, 6, 469, 3324, 14, 11, 6, 1, 11516, 338, 5, 1, 16023, 6870, 2543, 1061, 61649, 8, 2637, 1375, 20, 25365, 536, 33, 4727, 2520, 4, 1, 1208, 112, 31, 1, 7153, 25, 2992, 13015, 2, 408, 61650, 37, 17529, 6, 21, 319, 20, 1, 5098, 3720, 536, 6, 344, 5, 81744, 8470, 41120, 15379, 5171, 7893, 2461, 2, 18404, 61651, 329, 9265, 7472, 13445, 2, 8721, 34936, 23, 109, 224, 5436, 12, 9, 57, 128, 1, 269, 1303, 4, 1, 118, 6, 668, 5, 1, 187, 11, 8, 262, 112, 77, 257, 548, 3001, 819, 178, 1271, 4349, 16, 2499, 1096, 819, 1412, 819, 81745, 148, 978, 181, 1, 87, 393, 9, 120, 201, 3255, 69, 14, 37, 1574, 8, 13, 2214, 9, 397, 128, 9, 13, 1550, 16, 8, 18, 14, 9, 278, 51, 9, 1463, 3, 1250, 16, 3324, 2, 183, 10277, 5, 1, 319, 2092, 4, 2100, 582, 21, 40, 582, 18, 7965, 7154, 4974, 14178, 26, 2970, 45, 16, 3, 32611, 7035, 14178, 494, 20, 620, 2, 75, 240, 15, 8, 73, 9934, 753, 816, 7035, 106, 660, 81, 1208, 20591, 668, 5, 63, 549, 4, 931, 1996, 39, 1208, 558, 145, 3324, 22, 196, 411, 3778, 15, 48, 6, 3275, 81746, 43, 22, 68, 75, 7, 1211, 15, 122, 4018, 501]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels = np.array([1 if label == 'positive' else 0 for label in labels_split])"
      ],
      "metadata": {
        "id": "hILXdSEYmqy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outlier review stats\n",
        "review_lens = Counter([len(x) for x in reviews_ints])\n",
        "print(\"Zero-length reviews: {}\".format(review_lens[0]))\n",
        "print(\"Maximum review length: {}\".format(max(review_lens)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1O5C4-4mmq1l",
        "outputId": "7b6baf89-b4d7-4c34-981e-6050890a52c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Zero-length reviews: 0\n",
            "Maximum review length: 2469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of reviews before removing outliers: ', len(reviews_ints))\n",
        "\n",
        "non_zero_idx = [ii for ii, review in enumerate(reviews_ints) if len(review) != 0]\n",
        "\n",
        "# remove 0-length reviews and their labels\n",
        "reviews_ints = [reviews_ints[ii] for ii in non_zero_idx]\n",
        "encoded_labels = np.array([encoded_labels[ii] for ii in non_zero_idx])\n",
        "\n",
        "print('Number of reviews after removing outliers: ', len(reviews_ints))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19-aWz5pmq40",
        "outputId": "8df42a5a-8621-4d02-fd03-45dbd2a45924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reviews before removing outliers:  50000\n",
            "Number of reviews after removing outliers:  50000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding"
      ],
      "metadata": {
        "id": "cFycUfKRfZgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_features(reviews_ints, seq_length):\n",
        "\n",
        "\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "\n",
        "\n",
        "    for i, row in enumerate(reviews_ints):\n",
        "        features[i, :len(row)] = np.array(row)[:seq_length]\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "RMhTYkDPmq8N"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 200\n",
        "\n",
        "features = pad_features(reviews_ints, seq_length=seq_length)\n",
        "\n",
        "\n",
        "assert len(features)==len(reviews_ints), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "# printing first 10 values of the first 30 batches\n",
        "print(features[:30,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3922PHwsnmsd",
        "outputId": "5247af8d-d35e-4948-c707-f62fe4369b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[   28     4     1    77  1941    44  1063    11   100   145]\n",
            " [    3   384   115   358    12    12     1  1365  3015     6]\n",
            " [    9   194    10    13     3   384    98     5  1110    59]\n",
            " [  664   211     3   235   112     3   115   438  3554  1202]\n",
            " [81750 34937   110     7     1    59     4   291     6     3]\n",
            " [  234    53  3779   500    17     3    66     4 45600  4184]\n",
            " [    9   243    57    38     5    64     3  8960     4     3]\n",
            " [   10   118    13    33   488  1427  4082   313     7     1]\n",
            " [ 8471    32     1  1132   766    42    10    19    20   135]\n",
            " [   43    22    38   207  7529  9156  2209    22    76    38]\n",
            " [ 4482     1  1172     6    28     4   142  2578    95   112]\n",
            " [    9   201    10    17    50     9    13    42  1100    50]\n",
            " [   37   141    21     3   191   320     4 16024   163    18]\n",
            " [    1   179   248 25368    12  2195 12238    12     9  1074]\n",
            " [   10     3   793    17     4   279  3668    36   411   799]\n",
            " [  236     4  1537     7    32     1  2685   134    60     5]\n",
            " [   46    95    40   317   139    21    26  5409    10     6]\n",
            " [   10    17    92     8    81    28     4    53   401   298]\n",
            " [    9   372    10 13448    13     1    87    19     9    67]\n",
            " [   33   380    19     8   209    25    74    56   435    46]\n",
            " [  100     1  1009     4   800   261     2    29  2223    29]\n",
            " [    9    67     1   377  6412     4   255     5   656    10]\n",
            " [   48    33   403  1378    17    43    22    25  2493 14473]\n",
            " [   87     4    31   598    75     3   167   177   796   135]\n",
            " [   10    13     1   242    17     9   201    30 61666     2]\n",
            " [    1  3069  3593    66   257     3   115    51    42  1901]\n",
            " [    1  2639     6    33  4269   934     3 11684  1221    81]\n",
            " [   10    19   742     5    26    99   104   177    31    30]\n",
            " [   10    17    13    37  4527   263   448  6030     2     9]\n",
            " [  310    17     6     3   362   538    11    44    74   223]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting data into train, valid and test"
      ],
      "metadata": {
        "id": "Vqc-DeFZfck2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "# Splitting data into training, validation, and test data\n",
        "train_idx = int(len(features) * split_frac)\n",
        "train_x, remaining_x = features[:train_idx], features[train_idx:]\n",
        "train_y, remaining_y = encoded_labels[:train_idx], encoded_labels[train_idx:]\n",
        "\n",
        "test_val_idx = int(len(remaining_x) * 0.5)\n",
        "val_idx = test_val_idx\n",
        "test_idx = test_val_idx + int(len(remaining_x) * 0.5)\n",
        "\n",
        "val_x, test_x = remaining_x[:val_idx], remaining_x[val_idx:test_idx]\n",
        "val_y, test_y = remaining_y[:val_idx], remaining_y[val_idx:test_idx]\n",
        "\n",
        "\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPUHobkBnmv3",
        "outputId": "e54e3af7-0128-4720-f20a-871642086fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(40000, 200) \n",
            "Validation set: \t(5000, 200) \n",
            "Test set: \t\t(5000, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,  drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "BKmBRKoMnmzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', sample_x.size())\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size())\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uah8e0uE_6Kr",
        "outputId": "454c39b1-1d1b-4bb7-ee74-b003edea64a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([50, 200])\n",
            "Sample input: \n",
            " tensor([[  10,    6,   28,  ...,    0,    0,    0],\n",
            "        [  29,   52,  760,  ...,   59,    5, 1460],\n",
            "        [  48,    3,  862,  ...,    0,    0,    0],\n",
            "        ...,\n",
            "        [ 120,  231,    1,  ..., 8780,    2,    9],\n",
            "        [1319,   19,   15,  ...,    0,    0,    0],\n",
            "        [ 862,   62,   52,  ...,   65,   31,  509]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
            "        0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,\n",
            "        0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking if GPU is available\n",
        "train_on_gpu=torch.cuda.is_available()\n",
        "\n",
        "if(train_on_gpu):\n",
        "    print('Training on GPU.')\n",
        "else:\n",
        "    print('No GPU available, training on CPU.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZt6xciGrmP_",
        "outputId": "5c84f8b4-d7a7-431e-dc51-d7d7545f90ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, training on CPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM Network Architecture"
      ],
      "metadata": {
        "id": "EDsRTEC2fnys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentLSTM(nn.Module):\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        super(SentimentLSTM, self).__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
        "\n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = x.long()\n",
        "\n",
        "        # Embedding and LSTM layers\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out[:, -1, :]  # getting the last time step output\n",
        "\n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "\n",
        "\n",
        "        return sig_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        # Created two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
        "        else:\n",
        "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
        "\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "5lYXQee7r0E-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding size = 128\n",
        "hidden size = 128"
      ],
      "metadata": {
        "id": "8mRetmVqdLsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlueVgzX1GDf",
        "outputId": "de153baf-05fe-4750-dcc7-f87b8a101514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 128)\n",
            "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "8nF1bjTvf63J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5\n",
        "\n",
        "if train_on_gpu:\n",
        "    net_2.cuda()\n",
        "\n",
        "net_2.train()\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net_2.init_hidden(batch_size)\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if train_on_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "        output, h = net_2(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        net_2.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net_2.parameters(), clip)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "\n",
        "            val_h = net_2.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            net_2.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if train_on_gpu:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net_2(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                predicted = (output.squeeze() >= 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_acc = 100 * correct / total\n",
        "            net_2.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppf8AucZ1x0j",
        "outputId": "82989d93-1f8e-469a-a043-8c741364979c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 100... Loss: 0.697258... Val Loss: 0.693439 Val Acc: 49.940000\n",
            "Epoch: 1/20... Step: 200... Loss: 0.679065... Val Loss: 0.693294 Val Acc: 50.520000\n",
            "Epoch: 1/20... Step: 300... Loss: 0.720345... Val Loss: 0.686737 Val Acc: 53.380000\n",
            "Epoch: 1/20... Step: 400... Loss: 0.682507... Val Loss: 0.691381 Val Acc: 52.980000\n",
            "Epoch: 1/20... Step: 500... Loss: 0.682939... Val Loss: 0.686580 Val Acc: 53.120000\n",
            "Epoch: 1/20... Step: 600... Loss: 0.686275... Val Loss: 0.687061 Val Acc: 54.560000\n",
            "Epoch: 1/20... Step: 700... Loss: 0.690484... Val Loss: 0.693753 Val Acc: 50.780000\n",
            "Epoch: 1/20... Step: 800... Loss: 0.703117... Val Loss: 0.692999 Val Acc: 50.860000\n",
            "Epoch: 2/20... Step: 900... Loss: 0.700227... Val Loss: 0.692739 Val Acc: 50.860000\n",
            "Epoch: 2/20... Step: 1000... Loss: 0.684843... Val Loss: 0.689211 Val Acc: 55.060000\n",
            "Epoch: 2/20... Step: 1100... Loss: 0.692767... Val Loss: 0.693664 Val Acc: 50.120000\n",
            "Epoch: 2/20... Step: 1200... Loss: 0.691976... Val Loss: 0.687023 Val Acc: 54.240000\n",
            "Epoch: 2/20... Step: 1300... Loss: 0.678385... Val Loss: 0.672946 Val Acc: 56.620000\n",
            "Epoch: 2/20... Step: 1400... Loss: 0.651990... Val Loss: 0.677922 Val Acc: 55.400000\n",
            "Epoch: 2/20... Step: 1500... Loss: 0.621627... Val Loss: 0.674934 Val Acc: 55.900000\n",
            "Epoch: 2/20... Step: 1600... Loss: 0.686910... Val Loss: 0.690004 Val Acc: 52.540000\n",
            "Epoch: 3/20... Step: 1700... Loss: 0.690206... Val Loss: 0.705734 Val Acc: 50.240000\n",
            "Epoch: 3/20... Step: 1800... Loss: 0.713632... Val Loss: 0.692634 Val Acc: 51.320000\n",
            "Epoch: 3/20... Step: 1900... Loss: 0.685856... Val Loss: 0.672331 Val Acc: 59.300000\n",
            "Epoch: 3/20... Step: 2000... Loss: 0.756696... Val Loss: 0.685143 Val Acc: 58.300000\n",
            "Epoch: 3/20... Step: 2100... Loss: 0.518193... Val Loss: 0.623844 Val Acc: 66.440000\n",
            "Epoch: 3/20... Step: 2200... Loss: 0.486146... Val Loss: 0.578925 Val Acc: 72.240000\n",
            "Epoch: 3/20... Step: 2300... Loss: 0.504527... Val Loss: 0.523692 Val Acc: 74.840000\n",
            "Epoch: 3/20... Step: 2400... Loss: 0.594458... Val Loss: 0.558382 Val Acc: 70.320000\n",
            "Epoch: 4/20... Step: 2500... Loss: 0.444294... Val Loss: 0.466060 Val Acc: 78.160000\n",
            "Epoch: 4/20... Step: 2600... Loss: 0.453699... Val Loss: 0.524921 Val Acc: 77.300000\n",
            "Epoch: 4/20... Step: 2700... Loss: 0.431184... Val Loss: 0.489524 Val Acc: 77.340000\n",
            "Epoch: 4/20... Step: 2800... Loss: 0.431051... Val Loss: 0.446902 Val Acc: 78.420000\n",
            "Epoch: 4/20... Step: 2900... Loss: 0.369232... Val Loss: 0.440881 Val Acc: 79.600000\n",
            "Epoch: 4/20... Step: 3000... Loss: 0.332963... Val Loss: 0.447379 Val Acc: 78.260000\n",
            "Epoch: 4/20... Step: 3100... Loss: 0.348223... Val Loss: 0.428145 Val Acc: 81.020000\n",
            "Epoch: 4/20... Step: 3200... Loss: 0.363488... Val Loss: 0.416045 Val Acc: 81.500000\n",
            "Epoch: 5/20... Step: 3300... Loss: 0.403893... Val Loss: 0.490444 Val Acc: 79.440000\n",
            "Epoch: 5/20... Step: 3400... Loss: 0.267513... Val Loss: 0.450597 Val Acc: 80.140000\n",
            "Epoch: 5/20... Step: 3500... Loss: 0.192700... Val Loss: 0.434815 Val Acc: 81.560000\n",
            "Epoch: 5/20... Step: 3600... Loss: 0.544301... Val Loss: 0.442894 Val Acc: 82.060000\n",
            "Epoch: 5/20... Step: 3700... Loss: 0.509237... Val Loss: 0.446695 Val Acc: 81.800000\n",
            "Epoch: 5/20... Step: 3800... Loss: 0.307116... Val Loss: 0.410659 Val Acc: 82.940000\n",
            "Epoch: 5/20... Step: 3900... Loss: 0.373472... Val Loss: 0.403865 Val Acc: 82.780000\n",
            "Epoch: 5/20... Step: 4000... Loss: 0.226639... Val Loss: 0.391381 Val Acc: 82.360000\n",
            "Epoch: 6/20... Step: 4100... Loss: 0.317181... Val Loss: 0.419005 Val Acc: 83.420000\n",
            "Epoch: 6/20... Step: 4200... Loss: 0.378914... Val Loss: 0.426140 Val Acc: 83.600000\n",
            "Epoch: 6/20... Step: 4300... Loss: 0.335172... Val Loss: 0.405545 Val Acc: 83.320000\n",
            "Epoch: 6/20... Step: 4400... Loss: 0.183677... Val Loss: 0.407712 Val Acc: 82.980000\n",
            "Epoch: 6/20... Step: 4500... Loss: 0.298696... Val Loss: 0.398975 Val Acc: 83.400000\n",
            "Epoch: 6/20... Step: 4600... Loss: 0.264056... Val Loss: 0.420489 Val Acc: 83.420000\n",
            "Epoch: 6/20... Step: 4700... Loss: 0.460548... Val Loss: 0.383693 Val Acc: 84.020000\n",
            "Epoch: 6/20... Step: 4800... Loss: 0.246529... Val Loss: 0.403036 Val Acc: 84.400000\n",
            "Epoch: 7/20... Step: 4900... Loss: 0.239485... Val Loss: 0.395610 Val Acc: 84.400000\n",
            "Epoch: 7/20... Step: 5000... Loss: 0.102770... Val Loss: 0.416681 Val Acc: 84.540000\n",
            "Epoch: 7/20... Step: 5100... Loss: 0.053845... Val Loss: 0.441320 Val Acc: 84.460000\n",
            "Epoch: 7/20... Step: 5200... Loss: 0.199424... Val Loss: 0.412852 Val Acc: 84.480000\n",
            "Epoch: 7/20... Step: 5300... Loss: 0.321353... Val Loss: 0.420861 Val Acc: 84.560000\n",
            "Epoch: 7/20... Step: 5400... Loss: 0.320937... Val Loss: 0.404653 Val Acc: 84.580000\n",
            "Epoch: 7/20... Step: 5500... Loss: 0.278090... Val Loss: 0.412684 Val Acc: 84.040000\n",
            "Epoch: 7/20... Step: 5600... Loss: 0.172333... Val Loss: 0.396193 Val Acc: 84.900000\n",
            "Epoch: 8/20... Step: 5700... Loss: 0.159009... Val Loss: 0.403976 Val Acc: 84.700000\n",
            "Epoch: 8/20... Step: 5800... Loss: 0.113715... Val Loss: 0.420877 Val Acc: 84.560000\n",
            "Epoch: 8/20... Step: 5900... Loss: 0.212958... Val Loss: 0.427733 Val Acc: 83.780000\n",
            "Epoch: 8/20... Step: 6000... Loss: 0.119250... Val Loss: 0.424522 Val Acc: 84.660000\n",
            "Epoch: 8/20... Step: 6100... Loss: 0.102186... Val Loss: 0.416946 Val Acc: 83.640000\n",
            "Epoch: 8/20... Step: 6200... Loss: 0.244007... Val Loss: 0.407414 Val Acc: 83.760000\n",
            "Epoch: 8/20... Step: 6300... Loss: 0.070096... Val Loss: 0.393238 Val Acc: 84.520000\n",
            "Epoch: 8/20... Step: 6400... Loss: 0.250980... Val Loss: 0.422485 Val Acc: 84.420000\n",
            "Epoch: 9/20... Step: 6500... Loss: 0.305696... Val Loss: 0.460036 Val Acc: 84.980000\n",
            "Epoch: 9/20... Step: 6600... Loss: 0.102274... Val Loss: 0.431546 Val Acc: 85.240000\n",
            "Epoch: 9/20... Step: 6700... Loss: 0.164979... Val Loss: 0.441072 Val Acc: 85.100000\n",
            "Epoch: 9/20... Step: 6800... Loss: 0.172732... Val Loss: 0.506058 Val Acc: 83.600000\n",
            "Epoch: 9/20... Step: 6900... Loss: 0.143058... Val Loss: 0.447936 Val Acc: 84.740000\n",
            "Epoch: 9/20... Step: 7000... Loss: 0.218588... Val Loss: 0.449662 Val Acc: 85.380000\n",
            "Epoch: 9/20... Step: 7100... Loss: 0.303196... Val Loss: 0.448170 Val Acc: 84.940000\n",
            "Epoch: 9/20... Step: 7200... Loss: 0.076942... Val Loss: 0.424495 Val Acc: 84.340000\n",
            "Epoch: 10/20... Step: 7300... Loss: 0.126985... Val Loss: 0.478406 Val Acc: 84.840000\n",
            "Epoch: 10/20... Step: 7400... Loss: 0.268714... Val Loss: 0.485201 Val Acc: 85.300000\n",
            "Epoch: 10/20... Step: 7500... Loss: 0.148506... Val Loss: 0.461003 Val Acc: 85.180000\n",
            "Epoch: 10/20... Step: 7600... Loss: 0.099608... Val Loss: 0.450457 Val Acc: 84.900000\n",
            "Epoch: 10/20... Step: 7700... Loss: 0.190496... Val Loss: 0.482993 Val Acc: 84.440000\n",
            "Epoch: 10/20... Step: 7800... Loss: 0.058805... Val Loss: 0.474961 Val Acc: 84.560000\n",
            "Epoch: 10/20... Step: 7900... Loss: 0.123945... Val Loss: 0.518282 Val Acc: 84.540000\n",
            "Epoch: 10/20... Step: 8000... Loss: 0.171212... Val Loss: 0.486145 Val Acc: 85.160000\n",
            "Epoch: 11/20... Step: 8100... Loss: 0.027112... Val Loss: 0.515450 Val Acc: 85.320000\n",
            "Epoch: 11/20... Step: 8200... Loss: 0.200739... Val Loss: 0.496783 Val Acc: 85.360000\n",
            "Epoch: 11/20... Step: 8300... Loss: 0.011110... Val Loss: 0.525785 Val Acc: 85.360000\n",
            "Epoch: 11/20... Step: 8400... Loss: 0.066346... Val Loss: 0.496231 Val Acc: 84.340000\n",
            "Epoch: 11/20... Step: 8500... Loss: 0.140125... Val Loss: 0.502839 Val Acc: 85.200000\n",
            "Epoch: 11/20... Step: 8600... Loss: 0.128789... Val Loss: 0.472668 Val Acc: 84.940000\n",
            "Epoch: 11/20... Step: 8700... Loss: 0.030944... Val Loss: 0.496106 Val Acc: 85.140000\n",
            "Epoch: 11/20... Step: 8800... Loss: 0.015607... Val Loss: 0.516617 Val Acc: 85.240000\n",
            "Epoch: 12/20... Step: 8900... Loss: 0.022920... Val Loss: 0.541810 Val Acc: 85.480000\n",
            "Epoch: 12/20... Step: 9000... Loss: 0.155483... Val Loss: 0.552235 Val Acc: 84.420000\n",
            "Epoch: 12/20... Step: 9100... Loss: 0.041277... Val Loss: 0.515969 Val Acc: 85.420000\n",
            "Epoch: 12/20... Step: 9200... Loss: 0.014155... Val Loss: 0.509343 Val Acc: 84.920000\n",
            "Epoch: 12/20... Step: 9300... Loss: 0.181338... Val Loss: 0.502447 Val Acc: 85.280000\n",
            "Epoch: 12/20... Step: 9400... Loss: 0.034446... Val Loss: 0.543773 Val Acc: 85.620000\n",
            "Epoch: 12/20... Step: 9500... Loss: 0.107181... Val Loss: 0.547193 Val Acc: 85.400000\n",
            "Epoch: 12/20... Step: 9600... Loss: 0.054092... Val Loss: 0.509638 Val Acc: 85.420000\n",
            "Epoch: 13/20... Step: 9700... Loss: 0.007396... Val Loss: 0.561643 Val Acc: 84.880000\n",
            "Epoch: 13/20... Step: 9800... Loss: 0.013250... Val Loss: 0.567121 Val Acc: 85.100000\n",
            "Epoch: 13/20... Step: 9900... Loss: 0.019111... Val Loss: 0.558557 Val Acc: 84.740000\n",
            "Epoch: 13/20... Step: 10000... Loss: 0.234366... Val Loss: 0.572449 Val Acc: 85.140000\n",
            "Epoch: 13/20... Step: 10100... Loss: 0.061752... Val Loss: 0.542977 Val Acc: 85.420000\n",
            "Epoch: 13/20... Step: 10200... Loss: 0.087556... Val Loss: 0.518545 Val Acc: 85.460000\n",
            "Epoch: 13/20... Step: 10300... Loss: 0.076625... Val Loss: 0.595168 Val Acc: 84.980000\n",
            "Epoch: 13/20... Step: 10400... Loss: 0.083213... Val Loss: 0.570986 Val Acc: 85.140000\n",
            "Epoch: 14/20... Step: 10500... Loss: 0.012385... Val Loss: 0.602432 Val Acc: 84.400000\n",
            "Epoch: 14/20... Step: 10600... Loss: 0.008002... Val Loss: 0.583846 Val Acc: 84.680000\n",
            "Epoch: 14/20... Step: 10700... Loss: 0.061055... Val Loss: 0.570790 Val Acc: 84.880000\n",
            "Epoch: 14/20... Step: 10800... Loss: 0.070025... Val Loss: 0.598317 Val Acc: 85.180000\n",
            "Epoch: 14/20... Step: 10900... Loss: 0.078404... Val Loss: 0.574098 Val Acc: 85.100000\n",
            "Epoch: 14/20... Step: 11000... Loss: 0.013378... Val Loss: 0.570448 Val Acc: 85.260000\n",
            "Epoch: 14/20... Step: 11100... Loss: 0.078270... Val Loss: 0.564673 Val Acc: 85.600000\n",
            "Epoch: 14/20... Step: 11200... Loss: 0.012333... Val Loss: 0.553941 Val Acc: 85.100000\n",
            "Epoch: 15/20... Step: 11300... Loss: 0.006604... Val Loss: 0.592163 Val Acc: 85.140000\n",
            "Epoch: 15/20... Step: 11400... Loss: 0.012976... Val Loss: 0.593485 Val Acc: 85.260000\n",
            "Epoch: 15/20... Step: 11500... Loss: 0.140846... Val Loss: 0.594719 Val Acc: 85.680000\n",
            "Epoch: 15/20... Step: 11600... Loss: 0.003962... Val Loss: 0.597691 Val Acc: 85.280000\n",
            "Epoch: 15/20... Step: 11700... Loss: 0.014263... Val Loss: 0.649062 Val Acc: 84.800000\n",
            "Epoch: 15/20... Step: 11800... Loss: 0.005289... Val Loss: 0.621640 Val Acc: 84.580000\n",
            "Epoch: 15/20... Step: 11900... Loss: 0.093928... Val Loss: 0.587647 Val Acc: 83.500000\n",
            "Epoch: 15/20... Step: 12000... Loss: 0.028075... Val Loss: 0.606890 Val Acc: 85.260000\n",
            "Epoch: 16/20... Step: 12100... Loss: 0.006011... Val Loss: 0.596652 Val Acc: 84.780000\n",
            "Epoch: 16/20... Step: 12200... Loss: 0.129388... Val Loss: 0.684674 Val Acc: 84.440000\n",
            "Epoch: 16/20... Step: 12300... Loss: 0.122612... Val Loss: 0.637884 Val Acc: 83.320000\n",
            "Epoch: 16/20... Step: 12400... Loss: 0.003809... Val Loss: 0.637583 Val Acc: 85.440000\n",
            "Epoch: 16/20... Step: 12500... Loss: 0.004724... Val Loss: 0.669500 Val Acc: 85.280000\n",
            "Epoch: 16/20... Step: 12600... Loss: 0.014397... Val Loss: 0.654547 Val Acc: 84.960000\n",
            "Epoch: 16/20... Step: 12700... Loss: 0.189724... Val Loss: 0.618321 Val Acc: 85.900000\n",
            "Epoch: 16/20... Step: 12800... Loss: 0.005100... Val Loss: 0.620088 Val Acc: 84.420000\n",
            "Epoch: 17/20... Step: 12900... Loss: 0.012480... Val Loss: 0.641540 Val Acc: 85.540000\n",
            "Epoch: 17/20... Step: 13000... Loss: 0.002950... Val Loss: 0.704294 Val Acc: 84.540000\n",
            "Epoch: 17/20... Step: 13100... Loss: 0.002595... Val Loss: 0.723226 Val Acc: 83.900000\n",
            "Epoch: 17/20... Step: 13200... Loss: 0.003903... Val Loss: 0.654314 Val Acc: 85.560000\n",
            "Epoch: 17/20... Step: 13300... Loss: 0.079197... Val Loss: 0.608877 Val Acc: 84.560000\n",
            "Epoch: 17/20... Step: 13400... Loss: 0.004661... Val Loss: 0.651088 Val Acc: 84.880000\n",
            "Epoch: 17/20... Step: 13500... Loss: 0.005038... Val Loss: 0.668223 Val Acc: 84.880000\n",
            "Epoch: 17/20... Step: 13600... Loss: 0.019113... Val Loss: 0.701069 Val Acc: 83.900000\n",
            "Epoch: 18/20... Step: 13700... Loss: 0.004583... Val Loss: 0.759176 Val Acc: 84.140000\n",
            "Epoch: 18/20... Step: 13800... Loss: 0.008572... Val Loss: 0.625977 Val Acc: 83.800000\n",
            "Epoch: 18/20... Step: 13900... Loss: 0.076935... Val Loss: 0.658303 Val Acc: 84.680000\n",
            "Epoch: 18/20... Step: 14000... Loss: 0.003096... Val Loss: 0.703271 Val Acc: 84.580000\n",
            "Epoch: 18/20... Step: 14100... Loss: 0.111182... Val Loss: 0.630666 Val Acc: 84.980000\n",
            "Epoch: 18/20... Step: 14200... Loss: 0.002856... Val Loss: 0.668190 Val Acc: 85.320000\n",
            "Epoch: 18/20... Step: 14300... Loss: 0.012630... Val Loss: 0.610258 Val Acc: 85.060000\n",
            "Epoch: 18/20... Step: 14400... Loss: 0.012510... Val Loss: 0.684270 Val Acc: 84.400000\n",
            "Epoch: 19/20... Step: 14500... Loss: 0.003213... Val Loss: 0.733056 Val Acc: 83.980000\n",
            "Epoch: 19/20... Step: 14600... Loss: 0.001851... Val Loss: 0.725716 Val Acc: 84.720000\n",
            "Epoch: 19/20... Step: 14700... Loss: 0.140031... Val Loss: 0.755558 Val Acc: 84.320000\n",
            "Epoch: 19/20... Step: 14800... Loss: 0.001339... Val Loss: 0.755974 Val Acc: 84.680000\n",
            "Epoch: 19/20... Step: 14900... Loss: 0.009261... Val Loss: 0.686377 Val Acc: 84.760000\n",
            "Epoch: 19/20... Step: 15000... Loss: 0.090443... Val Loss: 0.640797 Val Acc: 83.960000\n",
            "Epoch: 19/20... Step: 15100... Loss: 0.002622... Val Loss: 0.736067 Val Acc: 85.120000\n",
            "Epoch: 19/20... Step: 15200... Loss: 0.002681... Val Loss: 0.750891 Val Acc: 84.560000\n",
            "Epoch: 20/20... Step: 15300... Loss: 0.005069... Val Loss: 0.719408 Val Acc: 84.380000\n",
            "Epoch: 20/20... Step: 15400... Loss: 0.003793... Val Loss: 0.670287 Val Acc: 85.080000\n",
            "Epoch: 20/20... Step: 15500... Loss: 0.007953... Val Loss: 0.674425 Val Acc: 84.420000\n",
            "Epoch: 20/20... Step: 15600... Loss: 0.018520... Val Loss: 0.750528 Val Acc: 83.680000\n",
            "Epoch: 20/20... Step: 15700... Loss: 0.063509... Val Loss: 0.665059 Val Acc: 85.100000\n",
            "Epoch: 20/20... Step: 15800... Loss: 0.006608... Val Loss: 0.721313 Val Acc: 84.920000\n",
            "Epoch: 20/20... Step: 15900... Loss: 0.006434... Val Loss: 0.691269 Val Acc: 84.520000\n",
            "Epoch: 20/20... Step: 16000... Loss: 0.003464... Val Loss: 0.713713 Val Acc: 84.660000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "7f8iCYVhf-GJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "h = net_2.init_hidden(batch_size)\n",
        "\n",
        "net_2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # getting predicted outputs\n",
        "    output, h = net_2(inputs, h)\n",
        "\n",
        "    # calculating loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dn90QwLY3LS3",
        "outputId": "9a42e094-cb3f-4858-cd28-b2809151c62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.760\n",
            "Test accuracy: 0.837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding size = 256\n",
        "hidden size = 128"
      ],
      "metadata": {
        "id": "FoBHTQRKq2ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 256\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iui00zmUrOH_",
        "outputId": "50a8d893-5da9-4cd7-bd07-768259a75be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 256)\n",
            "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "FIXSCEQKgFXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5\n",
        "\n",
        "if train_on_gpu:\n",
        "    net_2.cuda()\n",
        "\n",
        "net_2.train()\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net_2.init_hidden(batch_size)\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if train_on_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "\n",
        "        output, h = net_2(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        net_2.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net_2.parameters(), clip)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "\n",
        "            val_h = net_2.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            net_2.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if train_on_gpu:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net_2(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                predicted = (output.squeeze() >= 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_acc = 100 * correct / total\n",
        "            net_2.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ym7wxPcDrU_N",
        "outputId": "fd489a7c-b47a-474e-8e26-815254cf58b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 100... Loss: 0.702825... Val Loss: 0.693006 Val Acc: 51.020000\n",
            "Epoch: 1/20... Step: 200... Loss: 0.699039... Val Loss: 0.692240 Val Acc: 51.040000\n",
            "Epoch: 1/20... Step: 300... Loss: 0.660628... Val Loss: 0.687799 Val Acc: 52.760000\n",
            "Epoch: 1/20... Step: 400... Loss: 0.678751... Val Loss: 0.686601 Val Acc: 54.660000\n",
            "Epoch: 1/20... Step: 500... Loss: 0.736670... Val Loss: 0.676845 Val Acc: 56.880000\n",
            "Epoch: 1/20... Step: 600... Loss: 0.710945... Val Loss: 0.686574 Val Acc: 52.180000\n",
            "Epoch: 1/20... Step: 700... Loss: 0.694582... Val Loss: 0.692845 Val Acc: 50.820000\n",
            "Epoch: 1/20... Step: 800... Loss: 0.697468... Val Loss: 0.692918 Val Acc: 50.740000\n",
            "Epoch: 2/20... Step: 900... Loss: 0.690885... Val Loss: 0.693234 Val Acc: 50.640000\n",
            "Epoch: 2/20... Step: 1000... Loss: 0.695884... Val Loss: 0.693440 Val Acc: 50.180000\n",
            "Epoch: 2/20... Step: 1100... Loss: 0.695883... Val Loss: 0.693824 Val Acc: 49.800000\n",
            "Epoch: 2/20... Step: 1200... Loss: 0.693282... Val Loss: 0.693587 Val Acc: 51.020000\n",
            "Epoch: 2/20... Step: 1300... Loss: 0.694650... Val Loss: 0.692944 Val Acc: 50.900000\n",
            "Epoch: 2/20... Step: 1400... Loss: 0.691990... Val Loss: 0.692501 Val Acc: 51.380000\n",
            "Epoch: 2/20... Step: 1500... Loss: 0.646186... Val Loss: 0.646577 Val Acc: 66.800000\n",
            "Epoch: 2/20... Step: 1600... Loss: 0.528763... Val Loss: 0.590915 Val Acc: 70.860000\n",
            "Epoch: 3/20... Step: 1700... Loss: 0.522108... Val Loss: 0.567683 Val Acc: 73.440000\n",
            "Epoch: 3/20... Step: 1800... Loss: 0.560038... Val Loss: 0.590198 Val Acc: 71.860000\n",
            "Epoch: 3/20... Step: 1900... Loss: 0.519650... Val Loss: 0.562988 Val Acc: 74.240000\n",
            "Epoch: 3/20... Step: 2000... Loss: 0.683339... Val Loss: 0.520442 Val Acc: 76.320000\n",
            "Epoch: 3/20... Step: 2100... Loss: 0.527872... Val Loss: 0.496302 Val Acc: 77.020000\n",
            "Epoch: 3/20... Step: 2200... Loss: 0.514208... Val Loss: 0.473040 Val Acc: 78.820000\n",
            "Epoch: 3/20... Step: 2300... Loss: 0.439704... Val Loss: 0.473094 Val Acc: 79.440000\n",
            "Epoch: 3/20... Step: 2400... Loss: 0.538841... Val Loss: 0.460679 Val Acc: 79.340000\n",
            "Epoch: 4/20... Step: 2500... Loss: 0.507429... Val Loss: 0.454123 Val Acc: 80.980000\n",
            "Epoch: 4/20... Step: 2600... Loss: 0.422594... Val Loss: 0.485319 Val Acc: 80.340000\n",
            "Epoch: 4/20... Step: 2700... Loss: 0.370343... Val Loss: 0.449057 Val Acc: 80.780000\n",
            "Epoch: 4/20... Step: 2800... Loss: 0.294563... Val Loss: 0.431591 Val Acc: 82.120000\n",
            "Epoch: 4/20... Step: 2900... Loss: 0.486124... Val Loss: 0.426488 Val Acc: 81.880000\n",
            "Epoch: 4/20... Step: 3000... Loss: 0.245487... Val Loss: 0.421186 Val Acc: 82.460000\n",
            "Epoch: 4/20... Step: 3100... Loss: 0.388502... Val Loss: 0.409784 Val Acc: 83.460000\n",
            "Epoch: 4/20... Step: 3200... Loss: 0.215850... Val Loss: 0.394658 Val Acc: 83.740000\n",
            "Epoch: 5/20... Step: 3300... Loss: 0.218422... Val Loss: 0.404833 Val Acc: 83.820000\n",
            "Epoch: 5/20... Step: 3400... Loss: 0.282286... Val Loss: 0.440038 Val Acc: 83.100000\n",
            "Epoch: 5/20... Step: 3500... Loss: 0.242476... Val Loss: 0.418924 Val Acc: 83.920000\n",
            "Epoch: 5/20... Step: 3600... Loss: 0.170944... Val Loss: 0.438828 Val Acc: 83.760000\n",
            "Epoch: 5/20... Step: 3700... Loss: 0.318372... Val Loss: 0.387328 Val Acc: 84.500000\n",
            "Epoch: 5/20... Step: 3800... Loss: 0.211429... Val Loss: 0.380803 Val Acc: 84.020000\n",
            "Epoch: 5/20... Step: 3900... Loss: 0.300705... Val Loss: 0.397102 Val Acc: 83.600000\n",
            "Epoch: 5/20... Step: 4000... Loss: 0.280201... Val Loss: 0.409565 Val Acc: 84.160000\n",
            "Epoch: 6/20... Step: 4100... Loss: 0.094710... Val Loss: 0.451345 Val Acc: 84.000000\n",
            "Epoch: 6/20... Step: 4200... Loss: 0.291282... Val Loss: 0.416006 Val Acc: 84.360000\n",
            "Epoch: 6/20... Step: 4300... Loss: 0.085093... Val Loss: 0.427765 Val Acc: 84.600000\n",
            "Epoch: 6/20... Step: 4400... Loss: 0.153161... Val Loss: 0.420734 Val Acc: 84.260000\n",
            "Epoch: 6/20... Step: 4500... Loss: 0.240015... Val Loss: 0.421168 Val Acc: 84.600000\n",
            "Epoch: 6/20... Step: 4600... Loss: 0.313617... Val Loss: 0.400335 Val Acc: 84.980000\n",
            "Epoch: 6/20... Step: 4700... Loss: 0.098397... Val Loss: 0.473242 Val Acc: 82.340000\n",
            "Epoch: 6/20... Step: 4800... Loss: 0.136099... Val Loss: 0.395873 Val Acc: 85.180000\n",
            "Epoch: 7/20... Step: 4900... Loss: 0.122130... Val Loss: 0.460795 Val Acc: 84.840000\n",
            "Epoch: 7/20... Step: 5000... Loss: 0.058219... Val Loss: 0.471413 Val Acc: 84.340000\n",
            "Epoch: 7/20... Step: 5100... Loss: 0.188483... Val Loss: 0.473158 Val Acc: 85.160000\n",
            "Epoch: 7/20... Step: 5200... Loss: 0.204722... Val Loss: 0.428337 Val Acc: 84.980000\n",
            "Epoch: 7/20... Step: 5300... Loss: 0.180060... Val Loss: 0.424221 Val Acc: 84.960000\n",
            "Epoch: 7/20... Step: 5400... Loss: 0.267110... Val Loss: 0.487285 Val Acc: 83.860000\n",
            "Epoch: 7/20... Step: 5500... Loss: 0.071104... Val Loss: 0.438283 Val Acc: 84.980000\n",
            "Epoch: 7/20... Step: 5600... Loss: 0.283388... Val Loss: 0.453303 Val Acc: 83.920000\n",
            "Epoch: 8/20... Step: 5700... Loss: 0.053592... Val Loss: 0.521500 Val Acc: 84.400000\n",
            "Epoch: 8/20... Step: 5800... Loss: 0.054059... Val Loss: 0.501341 Val Acc: 85.200000\n",
            "Epoch: 8/20... Step: 5900... Loss: 0.122929... Val Loss: 0.517095 Val Acc: 84.420000\n",
            "Epoch: 8/20... Step: 6000... Loss: 0.028299... Val Loss: 0.506208 Val Acc: 84.440000\n",
            "Epoch: 8/20... Step: 6100... Loss: 0.062439... Val Loss: 0.521724 Val Acc: 85.040000\n",
            "Epoch: 8/20... Step: 6200... Loss: 0.113339... Val Loss: 0.507883 Val Acc: 85.340000\n",
            "Epoch: 8/20... Step: 6300... Loss: 0.017885... Val Loss: 0.554883 Val Acc: 84.840000\n",
            "Epoch: 8/20... Step: 6400... Loss: 0.106346... Val Loss: 0.557727 Val Acc: 84.300000\n",
            "Epoch: 9/20... Step: 6500... Loss: 0.054163... Val Loss: 0.547210 Val Acc: 84.460000\n",
            "Epoch: 9/20... Step: 6600... Loss: 0.141082... Val Loss: 0.578755 Val Acc: 84.180000\n",
            "Epoch: 9/20... Step: 6700... Loss: 0.030504... Val Loss: 0.603273 Val Acc: 84.600000\n",
            "Epoch: 9/20... Step: 6800... Loss: 0.056847... Val Loss: 0.600223 Val Acc: 84.500000\n",
            "Epoch: 9/20... Step: 6900... Loss: 0.114300... Val Loss: 0.569530 Val Acc: 84.580000\n",
            "Epoch: 9/20... Step: 7000... Loss: 0.043418... Val Loss: 0.562831 Val Acc: 84.640000\n",
            "Epoch: 9/20... Step: 7100... Loss: 0.075800... Val Loss: 0.533006 Val Acc: 84.840000\n",
            "Epoch: 9/20... Step: 7200... Loss: 0.108671... Val Loss: 0.572467 Val Acc: 84.300000\n",
            "Epoch: 10/20... Step: 7300... Loss: 0.061173... Val Loss: 0.624945 Val Acc: 84.480000\n",
            "Epoch: 10/20... Step: 7400... Loss: 0.005022... Val Loss: 0.673271 Val Acc: 84.680000\n",
            "Epoch: 10/20... Step: 7500... Loss: 0.018078... Val Loss: 0.625447 Val Acc: 84.680000\n",
            "Epoch: 10/20... Step: 7600... Loss: 0.008840... Val Loss: 0.592415 Val Acc: 85.080000\n",
            "Epoch: 10/20... Step: 7700... Loss: 0.118400... Val Loss: 0.677095 Val Acc: 83.840000\n",
            "Epoch: 10/20... Step: 7800... Loss: 0.059773... Val Loss: 0.687745 Val Acc: 84.480000\n",
            "Epoch: 10/20... Step: 7900... Loss: 0.046688... Val Loss: 0.641365 Val Acc: 84.220000\n",
            "Epoch: 10/20... Step: 8000... Loss: 0.167004... Val Loss: 0.585245 Val Acc: 84.060000\n",
            "Epoch: 11/20... Step: 8100... Loss: 0.125770... Val Loss: 0.725387 Val Acc: 84.120000\n",
            "Epoch: 11/20... Step: 8200... Loss: 0.045790... Val Loss: 0.643326 Val Acc: 84.400000\n",
            "Epoch: 11/20... Step: 8300... Loss: 0.003065... Val Loss: 0.718894 Val Acc: 84.420000\n",
            "Epoch: 11/20... Step: 8400... Loss: 0.012053... Val Loss: 0.636397 Val Acc: 84.340000\n",
            "Epoch: 11/20... Step: 8500... Loss: 0.012005... Val Loss: 0.641802 Val Acc: 84.560000\n",
            "Epoch: 11/20... Step: 8600... Loss: 0.034275... Val Loss: 0.686943 Val Acc: 83.780000\n",
            "Epoch: 11/20... Step: 8700... Loss: 0.044828... Val Loss: 0.633986 Val Acc: 83.540000\n",
            "Epoch: 11/20... Step: 8800... Loss: 0.014185... Val Loss: 0.639686 Val Acc: 84.760000\n",
            "Epoch: 12/20... Step: 8900... Loss: 0.006785... Val Loss: 0.695843 Val Acc: 84.160000\n",
            "Epoch: 12/20... Step: 9000... Loss: 0.060934... Val Loss: 0.728347 Val Acc: 83.600000\n",
            "Epoch: 12/20... Step: 9100... Loss: 0.005656... Val Loss: 0.687914 Val Acc: 84.540000\n",
            "Epoch: 12/20... Step: 9200... Loss: 0.024226... Val Loss: 0.726890 Val Acc: 83.740000\n",
            "Epoch: 12/20... Step: 9300... Loss: 0.001915... Val Loss: 0.775276 Val Acc: 84.240000\n",
            "Epoch: 12/20... Step: 9400... Loss: 0.032871... Val Loss: 0.694753 Val Acc: 84.160000\n",
            "Epoch: 12/20... Step: 9500... Loss: 0.005923... Val Loss: 0.632040 Val Acc: 84.380000\n",
            "Epoch: 12/20... Step: 9600... Loss: 0.157046... Val Loss: 0.706830 Val Acc: 84.460000\n",
            "Epoch: 13/20... Step: 9700... Loss: 0.028270... Val Loss: 0.687519 Val Acc: 84.060000\n",
            "Epoch: 13/20... Step: 9800... Loss: 0.026399... Val Loss: 0.807159 Val Acc: 84.080000\n",
            "Epoch: 13/20... Step: 9900... Loss: 0.002891... Val Loss: 0.719508 Val Acc: 84.140000\n",
            "Epoch: 13/20... Step: 10000... Loss: 0.103439... Val Loss: 0.769604 Val Acc: 83.040000\n",
            "Epoch: 13/20... Step: 10100... Loss: 0.039413... Val Loss: 0.796345 Val Acc: 82.620000\n",
            "Epoch: 13/20... Step: 10200... Loss: 0.006332... Val Loss: 0.668466 Val Acc: 83.940000\n",
            "Epoch: 13/20... Step: 10300... Loss: 0.048400... Val Loss: 0.726449 Val Acc: 84.000000\n",
            "Epoch: 13/20... Step: 10400... Loss: 0.011888... Val Loss: 0.738759 Val Acc: 84.220000\n",
            "Epoch: 14/20... Step: 10500... Loss: 0.001443... Val Loss: 0.850798 Val Acc: 84.520000\n",
            "Epoch: 14/20... Step: 10600... Loss: 0.122604... Val Loss: 0.817656 Val Acc: 84.240000\n",
            "Epoch: 14/20... Step: 10700... Loss: 0.003550... Val Loss: 0.746330 Val Acc: 83.700000\n",
            "Epoch: 14/20... Step: 10800... Loss: 0.007531... Val Loss: 0.764177 Val Acc: 83.700000\n",
            "Epoch: 14/20... Step: 10900... Loss: 0.015724... Val Loss: 0.751062 Val Acc: 83.300000\n",
            "Epoch: 14/20... Step: 11000... Loss: 0.007313... Val Loss: 0.781608 Val Acc: 83.540000\n",
            "Epoch: 14/20... Step: 11100... Loss: 0.014084... Val Loss: 0.698258 Val Acc: 83.900000\n",
            "Epoch: 14/20... Step: 11200... Loss: 0.005889... Val Loss: 0.714814 Val Acc: 83.760000\n",
            "Epoch: 15/20... Step: 11300... Loss: 0.002385... Val Loss: 0.899344 Val Acc: 82.960000\n",
            "Epoch: 15/20... Step: 11400... Loss: 0.077390... Val Loss: 0.777923 Val Acc: 84.320000\n",
            "Epoch: 15/20... Step: 11500... Loss: 0.082929... Val Loss: 0.784541 Val Acc: 83.840000\n",
            "Epoch: 15/20... Step: 11600... Loss: 0.011229... Val Loss: 0.799352 Val Acc: 82.920000\n",
            "Epoch: 15/20... Step: 11700... Loss: 0.049889... Val Loss: 0.906616 Val Acc: 83.180000\n",
            "Epoch: 15/20... Step: 11800... Loss: 0.018988... Val Loss: 0.766070 Val Acc: 83.620000\n",
            "Epoch: 15/20... Step: 11900... Loss: 0.003716... Val Loss: 0.818721 Val Acc: 83.860000\n",
            "Epoch: 15/20... Step: 12000... Loss: 0.007543... Val Loss: 0.733825 Val Acc: 83.520000\n",
            "Epoch: 16/20... Step: 12100... Loss: 0.001104... Val Loss: 0.965032 Val Acc: 83.120000\n",
            "Epoch: 16/20... Step: 12200... Loss: 0.020625... Val Loss: 0.836787 Val Acc: 83.180000\n",
            "Epoch: 16/20... Step: 12300... Loss: 0.001100... Val Loss: 0.887619 Val Acc: 83.140000\n",
            "Epoch: 16/20... Step: 12400... Loss: 0.002590... Val Loss: 0.868941 Val Acc: 83.300000\n",
            "Epoch: 16/20... Step: 12500... Loss: 0.052173... Val Loss: 0.835458 Val Acc: 83.520000\n",
            "Epoch: 16/20... Step: 12600... Loss: 0.016061... Val Loss: 0.825821 Val Acc: 83.700000\n",
            "Epoch: 16/20... Step: 12700... Loss: 0.000957... Val Loss: 0.916287 Val Acc: 83.240000\n",
            "Epoch: 16/20... Step: 12800... Loss: 0.024252... Val Loss: 0.835419 Val Acc: 83.700000\n",
            "Epoch: 17/20... Step: 12900... Loss: 0.001708... Val Loss: 0.882722 Val Acc: 82.860000\n",
            "Epoch: 17/20... Step: 13000... Loss: 0.015131... Val Loss: 0.915812 Val Acc: 83.200000\n",
            "Epoch: 17/20... Step: 13100... Loss: 0.011005... Val Loss: 1.000474 Val Acc: 83.400000\n",
            "Epoch: 17/20... Step: 13200... Loss: 0.000766... Val Loss: 0.893368 Val Acc: 83.380000\n",
            "Epoch: 17/20... Step: 13300... Loss: 0.008879... Val Loss: 0.684552 Val Acc: 83.840000\n",
            "Epoch: 17/20... Step: 13400... Loss: 0.061159... Val Loss: 0.822161 Val Acc: 83.260000\n",
            "Epoch: 17/20... Step: 13500... Loss: 0.004108... Val Loss: 0.908318 Val Acc: 83.040000\n",
            "Epoch: 17/20... Step: 13600... Loss: 0.008085... Val Loss: 0.872207 Val Acc: 83.700000\n",
            "Epoch: 18/20... Step: 13700... Loss: 0.012148... Val Loss: 0.880438 Val Acc: 83.740000\n",
            "Epoch: 18/20... Step: 13800... Loss: 0.000583... Val Loss: 0.957257 Val Acc: 83.520000\n",
            "Epoch: 18/20... Step: 13900... Loss: 0.143854... Val Loss: 0.858892 Val Acc: 83.040000\n",
            "Epoch: 18/20... Step: 14000... Loss: 0.001304... Val Loss: 0.835435 Val Acc: 83.200000\n",
            "Epoch: 18/20... Step: 14100... Loss: 0.035473... Val Loss: 0.834367 Val Acc: 83.580000\n",
            "Epoch: 18/20... Step: 14200... Loss: 0.002391... Val Loss: 0.866109 Val Acc: 84.120000\n",
            "Epoch: 18/20... Step: 14300... Loss: 0.000977... Val Loss: 0.998017 Val Acc: 82.620000\n",
            "Epoch: 18/20... Step: 14400... Loss: 0.002046... Val Loss: 0.875266 Val Acc: 83.600000\n",
            "Epoch: 19/20... Step: 14500... Loss: 0.000664... Val Loss: 0.838410 Val Acc: 83.680000\n",
            "Epoch: 19/20... Step: 14600... Loss: 0.001529... Val Loss: 0.963877 Val Acc: 83.420000\n",
            "Epoch: 19/20... Step: 14700... Loss: 0.001580... Val Loss: 0.886341 Val Acc: 84.300000\n",
            "Epoch: 19/20... Step: 14800... Loss: 0.000862... Val Loss: 0.959844 Val Acc: 83.780000\n",
            "Epoch: 19/20... Step: 14900... Loss: 0.004243... Val Loss: 0.929645 Val Acc: 82.960000\n",
            "Epoch: 19/20... Step: 15000... Loss: 0.011250... Val Loss: 0.946032 Val Acc: 82.660000\n",
            "Epoch: 19/20... Step: 15100... Loss: 0.002933... Val Loss: 1.013781 Val Acc: 82.680000\n",
            "Epoch: 19/20... Step: 15200... Loss: 0.000689... Val Loss: 0.973629 Val Acc: 83.300000\n",
            "Epoch: 20/20... Step: 15300... Loss: 0.012298... Val Loss: 0.927651 Val Acc: 83.340000\n",
            "Epoch: 20/20... Step: 15400... Loss: 0.000686... Val Loss: 1.048270 Val Acc: 82.880000\n",
            "Epoch: 20/20... Step: 15500... Loss: 0.000156... Val Loss: 1.104932 Val Acc: 83.880000\n",
            "Epoch: 20/20... Step: 15600... Loss: 0.000630... Val Loss: 1.064503 Val Acc: 83.380000\n",
            "Epoch: 20/20... Step: 15700... Loss: 0.000371... Val Loss: 1.068721 Val Acc: 83.440000\n",
            "Epoch: 20/20... Step: 15800... Loss: 0.002568... Val Loss: 1.002204 Val Acc: 83.520000\n",
            "Epoch: 20/20... Step: 15900... Loss: 0.000293... Val Loss: 0.980951 Val Acc: 84.040000\n",
            "Epoch: 20/20... Step: 16000... Loss: 0.017470... Val Loss: 0.941465 Val Acc: 83.720000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "UReAiwKlgJXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "h = net_2.init_hidden(batch_size)\n",
        "\n",
        "net_2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net_2(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbV6pe4xtdpz",
        "outputId": "f2952af8-6fcd-4ec9-a7ab-2be29b1a43c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.965\n",
            "Test accuracy: 0.829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding size = 512\n",
        "hidden size = 128"
      ],
      "metadata": {
        "id": "tL63tG6Wtxtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drXfCGTFt64Y",
        "outputId": "dcf0cdc5-0d9f-4f81-e6ff-69d811231f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 512)\n",
            "  (lstm): LSTM(512, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "7GSgdZtYgPRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5\n",
        "\n",
        "if train_on_gpu:\n",
        "    net_2.cuda()\n",
        "\n",
        "net_2.train()\n",
        "\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net_2.init_hidden(batch_size)\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if train_on_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "\n",
        "        output, h = net_2(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        net_2.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net_2.parameters(), clip)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "\n",
        "            val_h = net_2.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            net_2.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if train_on_gpu:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net_2(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                predicted = (output.squeeze() >= 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_acc = 100 * correct / total\n",
        "            net_2.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2EgmDVjt0H-",
        "outputId": "7b8bbd94-8f20-4daa-c838-624b2b94c1be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 100... Loss: 0.694617... Val Loss: 0.692430 Val Acc: 51.300000\n",
            "Epoch: 1/20... Step: 200... Loss: 0.683371... Val Loss: 0.686519 Val Acc: 54.560000\n",
            "Epoch: 1/20... Step: 300... Loss: 0.663677... Val Loss: 0.692235 Val Acc: 49.260000\n",
            "Epoch: 1/20... Step: 400... Loss: 0.635425... Val Loss: 0.643771 Val Acc: 63.820000\n",
            "Epoch: 1/20... Step: 500... Loss: 0.674976... Val Loss: 0.662361 Val Acc: 64.500000\n",
            "Epoch: 1/20... Step: 600... Loss: 0.675596... Val Loss: 0.640696 Val Acc: 66.420000\n",
            "Epoch: 1/20... Step: 700... Loss: 0.749199... Val Loss: 0.713838 Val Acc: 51.880000\n",
            "Epoch: 1/20... Step: 800... Loss: 0.517092... Val Loss: 0.605780 Val Acc: 67.040000\n",
            "Epoch: 2/20... Step: 900... Loss: 0.401648... Val Loss: 0.587018 Val Acc: 73.180000\n",
            "Epoch: 2/20... Step: 1000... Loss: 0.377664... Val Loss: 0.536553 Val Acc: 76.580000\n",
            "Epoch: 2/20... Step: 1100... Loss: 0.625742... Val Loss: 0.496142 Val Acc: 77.540000\n",
            "Epoch: 2/20... Step: 1200... Loss: 0.557960... Val Loss: 0.531953 Val Acc: 73.620000\n",
            "Epoch: 2/20... Step: 1300... Loss: 0.474073... Val Loss: 0.466239 Val Acc: 79.400000\n",
            "Epoch: 2/20... Step: 1400... Loss: 0.366918... Val Loss: 0.434629 Val Acc: 80.980000\n",
            "Epoch: 2/20... Step: 1500... Loss: 0.495318... Val Loss: 0.409747 Val Acc: 82.120000\n",
            "Epoch: 2/20... Step: 1600... Loss: 0.328743... Val Loss: 0.405355 Val Acc: 82.680000\n",
            "Epoch: 3/20... Step: 1700... Loss: 0.227001... Val Loss: 0.416616 Val Acc: 82.300000\n",
            "Epoch: 3/20... Step: 1800... Loss: 0.445276... Val Loss: 0.404710 Val Acc: 82.860000\n",
            "Epoch: 3/20... Step: 1900... Loss: 0.310939... Val Loss: 0.405081 Val Acc: 82.780000\n",
            "Epoch: 3/20... Step: 2000... Loss: 0.583382... Val Loss: 0.406202 Val Acc: 82.180000\n",
            "Epoch: 3/20... Step: 2100... Loss: 0.351015... Val Loss: 0.420630 Val Acc: 82.180000\n",
            "Epoch: 3/20... Step: 2200... Loss: 0.230881... Val Loss: 0.398545 Val Acc: 84.140000\n",
            "Epoch: 3/20... Step: 2300... Loss: 0.245141... Val Loss: 0.390503 Val Acc: 83.020000\n",
            "Epoch: 3/20... Step: 2400... Loss: 0.195993... Val Loss: 0.389681 Val Acc: 83.020000\n",
            "Epoch: 4/20... Step: 2500... Loss: 0.182866... Val Loss: 0.409317 Val Acc: 84.140000\n",
            "Epoch: 4/20... Step: 2600... Loss: 0.160587... Val Loss: 0.386416 Val Acc: 83.720000\n",
            "Epoch: 4/20... Step: 2700... Loss: 0.296646... Val Loss: 0.424350 Val Acc: 84.440000\n",
            "Epoch: 4/20... Step: 2800... Loss: 0.058014... Val Loss: 0.420908 Val Acc: 84.520000\n",
            "Epoch: 4/20... Step: 2900... Loss: 0.205752... Val Loss: 0.394335 Val Acc: 84.600000\n",
            "Epoch: 4/20... Step: 3000... Loss: 0.337455... Val Loss: 0.420393 Val Acc: 83.860000\n",
            "Epoch: 4/20... Step: 3100... Loss: 0.161960... Val Loss: 0.390859 Val Acc: 84.900000\n",
            "Epoch: 4/20... Step: 3200... Loss: 0.156673... Val Loss: 0.383745 Val Acc: 84.780000\n",
            "Epoch: 5/20... Step: 3300... Loss: 0.074101... Val Loss: 0.425775 Val Acc: 85.040000\n",
            "Epoch: 5/20... Step: 3400... Loss: 0.113407... Val Loss: 0.472488 Val Acc: 84.680000\n",
            "Epoch: 5/20... Step: 3500... Loss: 0.074133... Val Loss: 0.445771 Val Acc: 84.400000\n",
            "Epoch: 5/20... Step: 3600... Loss: 0.106147... Val Loss: 0.415357 Val Acc: 85.180000\n",
            "Epoch: 5/20... Step: 3700... Loss: 0.036234... Val Loss: 0.450184 Val Acc: 84.260000\n",
            "Epoch: 5/20... Step: 3800... Loss: 0.155404... Val Loss: 0.449433 Val Acc: 84.040000\n",
            "Epoch: 5/20... Step: 3900... Loss: 0.117181... Val Loss: 0.433201 Val Acc: 84.680000\n",
            "Epoch: 5/20... Step: 4000... Loss: 0.262781... Val Loss: 0.427604 Val Acc: 84.840000\n",
            "Epoch: 6/20... Step: 4100... Loss: 0.125913... Val Loss: 0.445026 Val Acc: 85.280000\n",
            "Epoch: 6/20... Step: 4200... Loss: 0.180777... Val Loss: 0.467012 Val Acc: 84.740000\n",
            "Epoch: 6/20... Step: 4300... Loss: 0.213388... Val Loss: 0.495999 Val Acc: 84.700000\n",
            "Epoch: 6/20... Step: 4400... Loss: 0.129797... Val Loss: 0.469418 Val Acc: 84.920000\n",
            "Epoch: 6/20... Step: 4500... Loss: 0.054059... Val Loss: 0.439727 Val Acc: 85.460000\n",
            "Epoch: 6/20... Step: 4600... Loss: 0.016225... Val Loss: 0.471928 Val Acc: 84.720000\n",
            "Epoch: 6/20... Step: 4700... Loss: 0.181859... Val Loss: 0.461308 Val Acc: 85.240000\n",
            "Epoch: 6/20... Step: 4800... Loss: 0.089105... Val Loss: 0.447934 Val Acc: 85.020000\n",
            "Epoch: 7/20... Step: 4900... Loss: 0.014730... Val Loss: 0.503908 Val Acc: 84.060000\n",
            "Epoch: 7/20... Step: 5000... Loss: 0.024727... Val Loss: 0.614450 Val Acc: 84.700000\n",
            "Epoch: 7/20... Step: 5100... Loss: 0.065104... Val Loss: 0.522613 Val Acc: 84.560000\n",
            "Epoch: 7/20... Step: 5200... Loss: 0.069410... Val Loss: 0.497502 Val Acc: 84.400000\n",
            "Epoch: 7/20... Step: 5300... Loss: 0.105029... Val Loss: 0.524681 Val Acc: 84.600000\n",
            "Epoch: 7/20... Step: 5400... Loss: 0.145051... Val Loss: 0.544944 Val Acc: 84.140000\n",
            "Epoch: 7/20... Step: 5500... Loss: 0.048190... Val Loss: 0.552981 Val Acc: 84.520000\n",
            "Epoch: 7/20... Step: 5600... Loss: 0.220978... Val Loss: 0.523895 Val Acc: 84.780000\n",
            "Epoch: 8/20... Step: 5700... Loss: 0.004133... Val Loss: 0.713403 Val Acc: 82.380000\n",
            "Epoch: 8/20... Step: 5800... Loss: 0.031501... Val Loss: 0.641350 Val Acc: 84.500000\n",
            "Epoch: 8/20... Step: 5900... Loss: 0.030942... Val Loss: 0.584580 Val Acc: 85.460000\n",
            "Epoch: 8/20... Step: 6000... Loss: 0.081431... Val Loss: 0.567460 Val Acc: 84.540000\n",
            "Epoch: 8/20... Step: 6100... Loss: 0.015715... Val Loss: 0.594649 Val Acc: 84.680000\n",
            "Epoch: 8/20... Step: 6200... Loss: 0.087574... Val Loss: 0.637591 Val Acc: 84.480000\n",
            "Epoch: 8/20... Step: 6300... Loss: 0.010040... Val Loss: 0.641884 Val Acc: 85.260000\n",
            "Epoch: 8/20... Step: 6400... Loss: 0.134912... Val Loss: 0.596710 Val Acc: 84.720000\n",
            "Epoch: 9/20... Step: 6500... Loss: 0.003552... Val Loss: 0.645636 Val Acc: 83.820000\n",
            "Epoch: 9/20... Step: 6600... Loss: 0.096122... Val Loss: 0.650295 Val Acc: 83.200000\n",
            "Epoch: 9/20... Step: 6700... Loss: 0.015782... Val Loss: 0.647290 Val Acc: 84.000000\n",
            "Epoch: 9/20... Step: 6800... Loss: 0.007655... Val Loss: 0.655862 Val Acc: 84.740000\n",
            "Epoch: 9/20... Step: 6900... Loss: 0.077810... Val Loss: 0.576664 Val Acc: 84.700000\n",
            "Epoch: 9/20... Step: 7000... Loss: 0.074284... Val Loss: 0.555226 Val Acc: 84.380000\n",
            "Epoch: 9/20... Step: 7100... Loss: 0.010134... Val Loss: 0.665521 Val Acc: 84.280000\n",
            "Epoch: 9/20... Step: 7200... Loss: 0.084685... Val Loss: 0.682003 Val Acc: 82.840000\n",
            "Epoch: 10/20... Step: 7300... Loss: 0.114828... Val Loss: 0.716166 Val Acc: 85.220000\n",
            "Epoch: 10/20... Step: 7400... Loss: 0.002997... Val Loss: 0.693310 Val Acc: 84.140000\n",
            "Epoch: 10/20... Step: 7500... Loss: 0.003497... Val Loss: 0.673475 Val Acc: 84.640000\n",
            "Epoch: 10/20... Step: 7600... Loss: 0.024778... Val Loss: 0.722503 Val Acc: 84.500000\n",
            "Epoch: 10/20... Step: 7700... Loss: 0.052797... Val Loss: 0.660789 Val Acc: 84.420000\n",
            "Epoch: 10/20... Step: 7800... Loss: 0.004068... Val Loss: 0.707636 Val Acc: 85.140000\n",
            "Epoch: 10/20... Step: 7900... Loss: 0.008977... Val Loss: 0.691372 Val Acc: 83.980000\n",
            "Epoch: 10/20... Step: 8000... Loss: 0.008813... Val Loss: 0.632789 Val Acc: 84.760000\n",
            "Epoch: 11/20... Step: 8100... Loss: 0.009579... Val Loss: 0.697869 Val Acc: 84.300000\n",
            "Epoch: 11/20... Step: 8200... Loss: 0.003728... Val Loss: 0.747426 Val Acc: 84.380000\n",
            "Epoch: 11/20... Step: 8300... Loss: 0.002258... Val Loss: 0.719770 Val Acc: 84.560000\n",
            "Epoch: 11/20... Step: 8400... Loss: 0.123551... Val Loss: 0.732820 Val Acc: 84.520000\n",
            "Epoch: 11/20... Step: 8500... Loss: 0.064126... Val Loss: 0.755049 Val Acc: 84.500000\n",
            "Epoch: 11/20... Step: 8600... Loss: 0.031088... Val Loss: 0.706286 Val Acc: 83.600000\n",
            "Epoch: 11/20... Step: 8700... Loss: 0.014411... Val Loss: 0.772036 Val Acc: 84.140000\n",
            "Epoch: 11/20... Step: 8800... Loss: 0.001800... Val Loss: 0.715360 Val Acc: 85.160000\n",
            "Epoch: 12/20... Step: 8900... Loss: 0.099840... Val Loss: 0.720428 Val Acc: 84.400000\n",
            "Epoch: 12/20... Step: 9000... Loss: 0.004151... Val Loss: 0.720326 Val Acc: 84.640000\n",
            "Epoch: 12/20... Step: 9100... Loss: 0.147642... Val Loss: 0.760398 Val Acc: 84.580000\n",
            "Epoch: 12/20... Step: 9200... Loss: 0.002284... Val Loss: 0.752505 Val Acc: 84.500000\n",
            "Epoch: 12/20... Step: 9300... Loss: 0.012514... Val Loss: 0.671961 Val Acc: 84.700000\n",
            "Epoch: 12/20... Step: 9400... Loss: 0.162983... Val Loss: 0.826893 Val Acc: 83.960000\n",
            "Epoch: 12/20... Step: 9500... Loss: 0.006951... Val Loss: 0.747161 Val Acc: 84.120000\n",
            "Epoch: 12/20... Step: 9600... Loss: 0.001943... Val Loss: 0.724473 Val Acc: 84.760000\n",
            "Epoch: 13/20... Step: 9700... Loss: 0.003276... Val Loss: 0.729473 Val Acc: 84.300000\n",
            "Epoch: 13/20... Step: 9800... Loss: 0.003653... Val Loss: 0.766269 Val Acc: 84.740000\n",
            "Epoch: 13/20... Step: 9900... Loss: 0.040346... Val Loss: 0.718434 Val Acc: 84.540000\n",
            "Epoch: 13/20... Step: 10000... Loss: 0.027847... Val Loss: 0.821407 Val Acc: 84.280000\n",
            "Epoch: 13/20... Step: 10100... Loss: 0.003800... Val Loss: 0.776177 Val Acc: 84.300000\n",
            "Epoch: 13/20... Step: 10200... Loss: 0.004640... Val Loss: 0.783507 Val Acc: 83.900000\n",
            "Epoch: 13/20... Step: 10300... Loss: 0.001681... Val Loss: 0.759064 Val Acc: 84.000000\n",
            "Epoch: 13/20... Step: 10400... Loss: 0.013640... Val Loss: 0.731208 Val Acc: 83.360000\n",
            "Epoch: 14/20... Step: 10500... Loss: 0.002379... Val Loss: 0.740721 Val Acc: 84.040000\n",
            "Epoch: 14/20... Step: 10600... Loss: 0.001860... Val Loss: 0.807214 Val Acc: 84.680000\n",
            "Epoch: 14/20... Step: 10700... Loss: 0.008042... Val Loss: 0.823773 Val Acc: 84.020000\n",
            "Epoch: 14/20... Step: 10800... Loss: 0.149955... Val Loss: 0.844668 Val Acc: 84.120000\n",
            "Epoch: 14/20... Step: 10900... Loss: 0.002871... Val Loss: 0.792593 Val Acc: 84.540000\n",
            "Epoch: 14/20... Step: 11000... Loss: 0.008689... Val Loss: 0.809801 Val Acc: 84.780000\n",
            "Epoch: 14/20... Step: 11100... Loss: 0.015095... Val Loss: 0.796690 Val Acc: 84.280000\n",
            "Epoch: 14/20... Step: 11200... Loss: 0.002364... Val Loss: 0.826517 Val Acc: 84.280000\n",
            "Epoch: 15/20... Step: 11300... Loss: 0.004430... Val Loss: 0.848695 Val Acc: 83.460000\n",
            "Epoch: 15/20... Step: 11400... Loss: 0.000392... Val Loss: 0.908409 Val Acc: 84.320000\n",
            "Epoch: 15/20... Step: 11500... Loss: 0.002885... Val Loss: 0.899549 Val Acc: 84.760000\n",
            "Epoch: 15/20... Step: 11600... Loss: 0.002061... Val Loss: 0.883193 Val Acc: 84.560000\n",
            "Epoch: 15/20... Step: 11700... Loss: 0.001446... Val Loss: 0.898944 Val Acc: 83.480000\n",
            "Epoch: 15/20... Step: 11800... Loss: 0.000913... Val Loss: 0.900573 Val Acc: 84.220000\n",
            "Epoch: 15/20... Step: 11900... Loss: 0.035306... Val Loss: 0.770671 Val Acc: 83.900000\n",
            "Epoch: 15/20... Step: 12000... Loss: 0.005837... Val Loss: 0.823228 Val Acc: 84.420000\n",
            "Epoch: 16/20... Step: 12100... Loss: 0.001523... Val Loss: 0.924715 Val Acc: 83.320000\n",
            "Epoch: 16/20... Step: 12200... Loss: 0.002859... Val Loss: 0.913902 Val Acc: 83.540000\n",
            "Epoch: 16/20... Step: 12300... Loss: 0.002578... Val Loss: 0.914892 Val Acc: 83.840000\n",
            "Epoch: 16/20... Step: 12400... Loss: 0.000815... Val Loss: 0.876002 Val Acc: 84.180000\n",
            "Epoch: 16/20... Step: 12500... Loss: 0.000985... Val Loss: 0.867772 Val Acc: 83.380000\n",
            "Epoch: 16/20... Step: 12600... Loss: 0.014657... Val Loss: 0.727959 Val Acc: 84.320000\n",
            "Epoch: 16/20... Step: 12700... Loss: 0.001737... Val Loss: 0.810324 Val Acc: 83.740000\n",
            "Epoch: 16/20... Step: 12800... Loss: 0.001643... Val Loss: 0.781617 Val Acc: 83.960000\n",
            "Epoch: 17/20... Step: 12900... Loss: 0.001140... Val Loss: 0.772968 Val Acc: 83.900000\n",
            "Epoch: 17/20... Step: 13000... Loss: 0.002873... Val Loss: 0.886433 Val Acc: 82.800000\n",
            "Epoch: 17/20... Step: 13100... Loss: 0.000438... Val Loss: 0.978037 Val Acc: 83.880000\n",
            "Epoch: 17/20... Step: 13200... Loss: 0.003367... Val Loss: 0.932326 Val Acc: 83.760000\n",
            "Epoch: 17/20... Step: 13300... Loss: 0.001888... Val Loss: 0.889668 Val Acc: 84.160000\n",
            "Epoch: 17/20... Step: 13400... Loss: 0.001668... Val Loss: 0.862613 Val Acc: 83.580000\n",
            "Epoch: 17/20... Step: 13500... Loss: 0.017912... Val Loss: 0.782899 Val Acc: 83.620000\n",
            "Epoch: 17/20... Step: 13600... Loss: 0.003033... Val Loss: 0.836152 Val Acc: 84.060000\n",
            "Epoch: 18/20... Step: 13700... Loss: 0.041971... Val Loss: 0.883227 Val Acc: 84.140000\n",
            "Epoch: 18/20... Step: 13800... Loss: 0.067333... Val Loss: 0.891853 Val Acc: 84.220000\n",
            "Epoch: 18/20... Step: 13900... Loss: 0.017502... Val Loss: 0.925692 Val Acc: 83.920000\n",
            "Epoch: 18/20... Step: 14000... Loss: 0.007063... Val Loss: 0.848349 Val Acc: 83.620000\n",
            "Epoch: 18/20... Step: 14100... Loss: 0.087838... Val Loss: 0.912276 Val Acc: 83.520000\n",
            "Epoch: 18/20... Step: 14200... Loss: 0.001454... Val Loss: 0.895555 Val Acc: 84.160000\n",
            "Epoch: 18/20... Step: 14300... Loss: 0.018199... Val Loss: 0.934635 Val Acc: 83.720000\n",
            "Epoch: 18/20... Step: 14400... Loss: 0.046058... Val Loss: 0.921797 Val Acc: 84.280000\n",
            "Epoch: 19/20... Step: 14500... Loss: 0.000537... Val Loss: 1.004760 Val Acc: 84.000000\n",
            "Epoch: 19/20... Step: 14600... Loss: 0.000897... Val Loss: 0.966045 Val Acc: 84.620000\n",
            "Epoch: 19/20... Step: 14700... Loss: 0.000354... Val Loss: 0.996013 Val Acc: 84.040000\n",
            "Epoch: 19/20... Step: 14800... Loss: 0.000828... Val Loss: 0.933434 Val Acc: 83.740000\n",
            "Epoch: 19/20... Step: 14900... Loss: 0.000514... Val Loss: 0.965418 Val Acc: 83.780000\n",
            "Epoch: 19/20... Step: 15000... Loss: 0.028496... Val Loss: 1.050188 Val Acc: 84.020000\n",
            "Epoch: 19/20... Step: 15100... Loss: 0.000711... Val Loss: 0.957265 Val Acc: 83.220000\n",
            "Epoch: 19/20... Step: 15200... Loss: 0.000646... Val Loss: 0.931755 Val Acc: 84.100000\n",
            "Epoch: 20/20... Step: 15300... Loss: 0.002623... Val Loss: 1.043763 Val Acc: 84.180000\n",
            "Epoch: 20/20... Step: 15400... Loss: 0.019149... Val Loss: 0.967465 Val Acc: 83.520000\n",
            "Epoch: 20/20... Step: 15500... Loss: 0.019591... Val Loss: 0.992968 Val Acc: 83.940000\n",
            "Epoch: 20/20... Step: 15600... Loss: 0.000878... Val Loss: 0.973124 Val Acc: 84.120000\n",
            "Epoch: 20/20... Step: 15700... Loss: 0.005542... Val Loss: 0.976741 Val Acc: 83.320000\n",
            "Epoch: 20/20... Step: 15800... Loss: 0.000261... Val Loss: 1.017246 Val Acc: 84.440000\n",
            "Epoch: 20/20... Step: 15900... Loss: 0.005098... Val Loss: 1.035529 Val Acc: 82.460000\n",
            "Epoch: 20/20... Step: 16000... Loss: 0.000620... Val Loss: 1.008546 Val Acc: 83.760000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "o2y2-rlxgTJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "\n",
        "h = net_2.init_hidden(batch_size)\n",
        "\n",
        "net_2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net_2(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ISZR99Ktw-o",
        "outputId": "d053bbb8-da07-4d5b-9650-2562b03d563f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.980\n",
            "Test accuracy: 0.842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding size = 128\n",
        "hidden size = 256"
      ],
      "metadata": {
        "id": "YPfuRxSMynwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJeDnns0y5uJ",
        "outputId": "2f0c3a86-7910-4ef0-c5d2-c939a8f4e3b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 128)\n",
            "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_lstm(net_2):\n",
        "  epochs = 15\n",
        "  counter = 0\n",
        "  print_every = 100\n",
        "  clip = 5\n",
        "\n",
        "  if train_on_gpu:\n",
        "      net_2.cuda()\n",
        "\n",
        "  net_2.train()\n",
        "\n",
        "  for e in range(epochs):\n",
        "      # initialize hidden state\n",
        "      h = net_2.init_hidden(batch_size)\n",
        "\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          counter += 1\n",
        "\n",
        "          if train_on_gpu:\n",
        "              inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "          # Creating new variables for the hidden state\n",
        "          h = tuple([each.data for each in h])\n",
        "\n",
        "\n",
        "          output, h = net_2(inputs, h)\n",
        "\n",
        "          loss = criterion(output.squeeze(), labels.float())\n",
        "          net_2.zero_grad()\n",
        "\n",
        "          # Backpropagation\n",
        "          loss.backward()\n",
        "\n",
        "          nn.utils.clip_grad_norm_(net_2.parameters(), clip)\n",
        "\n",
        "          # Update weights\n",
        "          optimizer.step()\n",
        "\n",
        "          # loss stats\n",
        "          if counter % print_every == 0:\n",
        "\n",
        "              val_h = net_2.init_hidden(batch_size)\n",
        "              val_losses = []\n",
        "              correct = 0\n",
        "              total = 0\n",
        "              net_2.eval()\n",
        "              for inputs, labels in valid_loader:\n",
        "\n",
        "                  # Creating new variables for the hidden state\n",
        "                  val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                  if train_on_gpu:\n",
        "                      inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                  output, val_h = net_2(inputs, val_h)\n",
        "                  val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                  val_losses.append(val_loss.item())\n",
        "                  predicted = (output.squeeze() >= 0.5).float()\n",
        "                  total += labels.size(0)\n",
        "                  correct += (predicted == labels).sum().item()\n",
        "\n",
        "              val_acc = 100 * correct / total\n",
        "              net_2.train()\n",
        "              print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                    \"Step: {}...\".format(counter),\n",
        "                    \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                    \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                    \"Val Acc: {:.6f}\".format(val_acc))\n",
        "  return net_2\n"
      ],
      "metadata": {
        "id": "W4FeY6vQytyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "6Z6IbufthWqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPSl8sdJ2Twr",
        "outputId": "61de72a8-8dc9-42cd-f036-8a54d6f0ad71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.688641... Val Loss: 0.693799 Val Acc: 50.220000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.687708... Val Loss: 0.691927 Val Acc: 50.680000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.684072... Val Loss: 0.693244 Val Acc: 50.900000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.685174... Val Loss: 0.698042 Val Acc: 49.440000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.699803... Val Loss: 0.693066 Val Acc: 50.300000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.693471... Val Loss: 0.693275 Val Acc: 50.160000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.693082... Val Loss: 0.693078 Val Acc: 50.740000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.694457... Val Loss: 0.694196 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.695498... Val Loss: 0.692015 Val Acc: 52.160000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.687206... Val Loss: 0.689434 Val Acc: 53.260000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.679238... Val Loss: 0.670346 Val Acc: 59.800000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.696126... Val Loss: 0.699625 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.692710... Val Loss: 0.692138 Val Acc: 50.420000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.691861... Val Loss: 0.691663 Val Acc: 50.320000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.691141... Val Loss: 0.693364 Val Acc: 49.520000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.688079... Val Loss: 0.692321 Val Acc: 52.180000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.683519... Val Loss: 0.691591 Val Acc: 51.540000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.676900... Val Loss: 0.686457 Val Acc: 55.260000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.675650... Val Loss: 0.695633 Val Acc: 51.660000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.671279... Val Loss: 0.693772 Val Acc: 51.360000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.669328... Val Loss: 0.691153 Val Acc: 52.500000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.687932... Val Loss: 0.687229 Val Acc: 54.500000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.736686... Val Loss: 0.709819 Val Acc: 50.920000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.676546... Val Loss: 0.699274 Val Acc: 49.260000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.649523... Val Loss: 0.679488 Val Acc: 55.200000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.686398... Val Loss: 0.693254 Val Acc: 50.540000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.685846... Val Loss: 0.692944 Val Acc: 49.900000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.700267... Val Loss: 0.692503 Val Acc: 52.160000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.693859... Val Loss: 0.689328 Val Acc: 52.480000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.666235... Val Loss: 0.693421 Val Acc: 52.820000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.650836... Val Loss: 0.674288 Val Acc: 60.520000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.656303... Val Loss: 0.708909 Val Acc: 53.780000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.569122... Val Loss: 0.561241 Val Acc: 74.300000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.580708... Val Loss: 0.534262 Val Acc: 74.740000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.480619... Val Loss: 0.495950 Val Acc: 78.020000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.470730... Val Loss: 0.543347 Val Acc: 74.040000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.477468... Val Loss: 0.466429 Val Acc: 77.500000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.334040... Val Loss: 0.484300 Val Acc: 77.900000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.198878... Val Loss: 0.429871 Val Acc: 81.140000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.400135... Val Loss: 0.408918 Val Acc: 81.620000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.264910... Val Loss: 0.398103 Val Acc: 83.400000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.451174... Val Loss: 0.389586 Val Acc: 82.620000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.398445... Val Loss: 0.388277 Val Acc: 82.240000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.303982... Val Loss: 0.375760 Val Acc: 83.160000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.294687... Val Loss: 0.381091 Val Acc: 83.240000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.357808... Val Loss: 0.384704 Val Acc: 84.200000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.205843... Val Loss: 0.363339 Val Acc: 84.720000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.284254... Val Loss: 0.373450 Val Acc: 84.160000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.302542... Val Loss: 0.368303 Val Acc: 84.340000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.370417... Val Loss: 0.392418 Val Acc: 85.280000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.210548... Val Loss: 0.376869 Val Acc: 83.940000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.146731... Val Loss: 0.388728 Val Acc: 85.040000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.181525... Val Loss: 0.373062 Val Acc: 84.820000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.184536... Val Loss: 0.361170 Val Acc: 85.200000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.121754... Val Loss: 0.364291 Val Acc: 84.960000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.204479... Val Loss: 0.344323 Val Acc: 85.140000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.087864... Val Loss: 0.376823 Val Acc: 84.780000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.143314... Val Loss: 0.422843 Val Acc: 85.300000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.178218... Val Loss: 0.385222 Val Acc: 85.600000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.136844... Val Loss: 0.379124 Val Acc: 85.020000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.213386... Val Loss: 0.387580 Val Acc: 85.020000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.114340... Val Loss: 0.360745 Val Acc: 85.420000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.272992... Val Loss: 0.370433 Val Acc: 85.400000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.103624... Val Loss: 0.401246 Val Acc: 85.620000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.091466... Val Loss: 0.429582 Val Acc: 85.480000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.065763... Val Loss: 0.438127 Val Acc: 85.820000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.060610... Val Loss: 0.423328 Val Acc: 85.740000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.103110... Val Loss: 0.417930 Val Acc: 85.300000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.074445... Val Loss: 0.406899 Val Acc: 85.940000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.114408... Val Loss: 0.450950 Val Acc: 85.660000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.047766... Val Loss: 0.459528 Val Acc: 84.900000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.117735... Val Loss: 0.402165 Val Acc: 85.960000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.058555... Val Loss: 0.468395 Val Acc: 85.360000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.098146... Val Loss: 0.448461 Val Acc: 85.300000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.251016... Val Loss: 0.418759 Val Acc: 85.740000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.121668... Val Loss: 0.472206 Val Acc: 85.400000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.055592... Val Loss: 0.459490 Val Acc: 85.960000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.043609... Val Loss: 0.485888 Val Acc: 84.880000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.356332... Val Loss: 0.488991 Val Acc: 85.780000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.142014... Val Loss: 0.440431 Val Acc: 86.120000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.021764... Val Loss: 0.516229 Val Acc: 85.580000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.030733... Val Loss: 0.505781 Val Acc: 85.240000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.021995... Val Loss: 0.508912 Val Acc: 85.640000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.053754... Val Loss: 0.471064 Val Acc: 85.840000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.132664... Val Loss: 0.487784 Val Acc: 85.560000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.065271... Val Loss: 0.535570 Val Acc: 84.740000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.034122... Val Loss: 0.487714 Val Acc: 85.460000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.021968... Val Loss: 0.514868 Val Acc: 83.780000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.011235... Val Loss: 0.558179 Val Acc: 84.840000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.024568... Val Loss: 0.502773 Val Acc: 85.300000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.021100... Val Loss: 0.516935 Val Acc: 85.400000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.089625... Val Loss: 0.574483 Val Acc: 84.960000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.019908... Val Loss: 0.567134 Val Acc: 85.080000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.026258... Val Loss: 0.533341 Val Acc: 85.500000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.050401... Val Loss: 0.491587 Val Acc: 86.240000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.064472... Val Loss: 0.514042 Val Acc: 85.900000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.140735... Val Loss: 0.566397 Val Acc: 85.280000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.015863... Val Loss: 0.549725 Val Acc: 85.160000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.025832... Val Loss: 0.590038 Val Acc: 84.820000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.165902... Val Loss: 0.575858 Val Acc: 85.380000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.057971... Val Loss: 0.602521 Val Acc: 85.680000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.011355... Val Loss: 0.569612 Val Acc: 85.240000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.006922... Val Loss: 0.594885 Val Acc: 85.560000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.174617... Val Loss: 0.574796 Val Acc: 85.200000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.049336... Val Loss: 0.600319 Val Acc: 85.900000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.001727... Val Loss: 0.677803 Val Acc: 85.500000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.004843... Val Loss: 0.576865 Val Acc: 85.420000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.022953... Val Loss: 0.584948 Val Acc: 85.020000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.004328... Val Loss: 0.681926 Val Acc: 85.040000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.025031... Val Loss: 0.563392 Val Acc: 85.520000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.204144... Val Loss: 0.598740 Val Acc: 84.920000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.002962... Val Loss: 0.644729 Val Acc: 85.620000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.181709... Val Loss: 0.694943 Val Acc: 85.340000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.152700... Val Loss: 0.735305 Val Acc: 84.840000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.005410... Val Loss: 0.660358 Val Acc: 85.240000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.027020... Val Loss: 0.637353 Val Acc: 84.700000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.004768... Val Loss: 0.644938 Val Acc: 85.300000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.004165... Val Loss: 0.708547 Val Acc: 84.440000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.007239... Val Loss: 0.649292 Val Acc: 84.800000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.002497... Val Loss: 0.768831 Val Acc: 84.640000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "7i34oV0JhZ1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_losses = []\n",
        "num_correct = 0\n",
        "\n",
        "h = net_2.init_hidden(batch_size)\n",
        "\n",
        "net_2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net_2(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4QurLYJysOD",
        "outputId": "159e0d2d-7d13-4446-e5dc-389670084027"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.765\n",
            "Test accuracy: 0.844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding dim = 256\n",
        "hidden dim = 256"
      ],
      "metadata": {
        "id": "PfB76cF64_iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 256\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Am9oyzoI5Wmn",
        "outputId": "9113fbad-92bb-45de-a4be-7b9dcb0f0b75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 256)\n",
            "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "YeJfgK8ghi30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giXRzoMN5fkg",
        "outputId": "2a323eae-8640-4513-ac61-44f94419ec01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.688169... Val Loss: 0.703493 Val Acc: 49.820000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.689537... Val Loss: 0.693383 Val Acc: 49.540000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.696504... Val Loss: 0.691002 Val Acc: 52.320000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.686226... Val Loss: 0.693581 Val Acc: 49.760000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.690426... Val Loss: 0.688194 Val Acc: 55.600000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.679649... Val Loss: 0.688669 Val Acc: 55.160000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.693482... Val Loss: 0.694542 Val Acc: 49.260000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.691703... Val Loss: 0.692834 Val Acc: 51.420000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.692418... Val Loss: 0.693024 Val Acc: 50.180000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.694951... Val Loss: 0.692768 Val Acc: 50.740000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.685779... Val Loss: 0.693102 Val Acc: 50.740000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.689038... Val Loss: 0.692941 Val Acc: 50.260000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.692532... Val Loss: 0.693308 Val Acc: 50.520000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.697189... Val Loss: 0.692919 Val Acc: 50.560000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.689651... Val Loss: 0.693379 Val Acc: 50.740000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.692018... Val Loss: 0.692514 Val Acc: 52.380000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.685605... Val Loss: 0.689739 Val Acc: 54.100000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.696430... Val Loss: 0.692148 Val Acc: 50.940000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.690061... Val Loss: 0.688342 Val Acc: 53.100000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.696066... Val Loss: 0.692873 Val Acc: 50.760000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.694523... Val Loss: 0.683278 Val Acc: 55.160000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.650945... Val Loss: 0.672679 Val Acc: 58.420000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.659805... Val Loss: 0.676288 Val Acc: 58.600000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.686156... Val Loss: 0.611844 Val Acc: 67.140000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.476234... Val Loss: 0.512411 Val Acc: 76.300000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.448456... Val Loss: 0.497064 Val Acc: 75.960000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.437860... Val Loss: 0.461782 Val Acc: 79.260000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.458155... Val Loss: 0.448087 Val Acc: 80.120000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.362662... Val Loss: 0.441106 Val Acc: 80.080000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.439471... Val Loss: 0.437197 Val Acc: 79.460000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.358361... Val Loss: 0.404523 Val Acc: 82.300000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.449608... Val Loss: 0.441222 Val Acc: 81.040000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.193129... Val Loss: 0.431650 Val Acc: 82.020000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.352859... Val Loss: 0.392809 Val Acc: 83.080000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.214202... Val Loss: 0.437706 Val Acc: 82.740000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.276970... Val Loss: 0.393960 Val Acc: 83.700000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.257818... Val Loss: 0.387415 Val Acc: 83.300000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.128313... Val Loss: 0.384860 Val Acc: 84.080000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.302553... Val Loss: 0.402916 Val Acc: 83.560000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.268884... Val Loss: 0.367886 Val Acc: 84.760000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.124370... Val Loss: 0.377136 Val Acc: 84.680000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.110994... Val Loss: 0.409056 Val Acc: 85.060000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.281044... Val Loss: 0.395595 Val Acc: 84.940000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.253940... Val Loss: 0.405334 Val Acc: 84.940000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.232502... Val Loss: 0.394885 Val Acc: 84.080000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.495846... Val Loss: 0.387820 Val Acc: 84.560000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.151737... Val Loss: 0.399906 Val Acc: 85.400000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.174034... Val Loss: 0.373935 Val Acc: 84.640000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.129168... Val Loss: 0.413009 Val Acc: 84.860000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.053753... Val Loss: 0.401255 Val Acc: 84.700000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.060068... Val Loss: 0.394166 Val Acc: 85.340000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.238515... Val Loss: 0.414889 Val Acc: 85.640000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.059300... Val Loss: 0.418816 Val Acc: 84.920000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.060734... Val Loss: 0.428235 Val Acc: 85.400000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.260645... Val Loss: 0.438582 Val Acc: 85.080000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.094983... Val Loss: 0.409849 Val Acc: 84.800000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.041789... Val Loss: 0.458910 Val Acc: 85.360000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.187838... Val Loss: 0.465615 Val Acc: 85.500000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.068739... Val Loss: 0.459900 Val Acc: 85.580000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.150554... Val Loss: 0.459844 Val Acc: 85.460000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.047004... Val Loss: 0.441651 Val Acc: 85.160000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.149852... Val Loss: 0.465776 Val Acc: 85.080000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.027977... Val Loss: 0.452764 Val Acc: 85.520000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.187542... Val Loss: 0.452394 Val Acc: 84.640000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.023780... Val Loss: 0.567107 Val Acc: 85.200000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.093466... Val Loss: 0.482475 Val Acc: 85.560000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.058420... Val Loss: 0.509257 Val Acc: 85.080000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.021098... Val Loss: 0.558190 Val Acc: 84.820000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.053546... Val Loss: 0.523238 Val Acc: 85.020000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.158510... Val Loss: 0.532442 Val Acc: 85.600000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.037937... Val Loss: 0.534997 Val Acc: 85.100000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.023581... Val Loss: 0.529180 Val Acc: 85.100000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.043242... Val Loss: 0.557580 Val Acc: 84.880000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.084125... Val Loss: 0.575035 Val Acc: 84.900000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.047110... Val Loss: 0.612664 Val Acc: 85.020000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.037860... Val Loss: 0.575683 Val Acc: 85.080000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.033074... Val Loss: 0.587654 Val Acc: 84.540000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.038871... Val Loss: 0.587698 Val Acc: 85.620000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.014426... Val Loss: 0.622016 Val Acc: 83.100000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.016298... Val Loss: 0.642315 Val Acc: 83.980000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.049095... Val Loss: 0.589710 Val Acc: 84.320000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.024111... Val Loss: 0.624420 Val Acc: 83.820000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.004272... Val Loss: 0.632273 Val Acc: 84.420000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.022996... Val Loss: 0.683886 Val Acc: 84.640000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.211891... Val Loss: 0.623185 Val Acc: 84.380000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.048864... Val Loss: 0.642413 Val Acc: 85.040000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.012000... Val Loss: 0.657483 Val Acc: 84.980000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.008062... Val Loss: 0.575694 Val Acc: 84.780000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.002078... Val Loss: 0.701344 Val Acc: 85.060000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.132820... Val Loss: 0.739928 Val Acc: 83.420000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.036952... Val Loss: 0.665067 Val Acc: 85.100000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.005207... Val Loss: 0.703579 Val Acc: 84.780000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.028097... Val Loss: 0.689596 Val Acc: 83.860000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.014108... Val Loss: 0.591310 Val Acc: 84.220000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.005052... Val Loss: 0.600812 Val Acc: 84.900000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.086386... Val Loss: 0.623971 Val Acc: 84.680000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.003401... Val Loss: 0.764661 Val Acc: 84.880000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.006461... Val Loss: 0.676584 Val Acc: 84.600000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.005005... Val Loss: 0.733029 Val Acc: 84.760000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.014709... Val Loss: 0.659911 Val Acc: 84.700000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.001975... Val Loss: 0.746024 Val Acc: 84.720000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.020437... Val Loss: 0.682944 Val Acc: 84.240000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.167838... Val Loss: 0.700498 Val Acc: 84.200000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.004244... Val Loss: 0.680221 Val Acc: 84.800000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.009789... Val Loss: 0.685693 Val Acc: 84.340000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.001879... Val Loss: 0.733773 Val Acc: 84.960000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.003120... Val Loss: 0.686319 Val Acc: 84.600000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.072822... Val Loss: 0.775656 Val Acc: 84.800000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.106777... Val Loss: 0.774736 Val Acc: 85.000000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.091847... Val Loss: 0.750072 Val Acc: 83.600000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.001388... Val Loss: 0.744332 Val Acc: 84.480000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.002851... Val Loss: 0.721599 Val Acc: 84.500000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.005763... Val Loss: 0.715676 Val Acc: 84.360000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.003338... Val Loss: 0.794534 Val Acc: 84.540000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.029652... Val Loss: 0.782392 Val Acc: 84.460000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.082815... Val Loss: 0.765861 Val Acc: 83.900000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.002025... Val Loss: 0.757634 Val Acc: 84.140000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.002471... Val Loss: 0.699851 Val Acc: 84.600000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.037742... Val Loss: 0.707604 Val Acc: 85.040000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.037402... Val Loss: 0.778199 Val Acc: 85.020000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_lstm(net_2):\n",
        "  test_losses = []\n",
        "  num_correct = 0\n",
        "\n",
        "  h = net_2.init_hidden(batch_size)\n",
        "\n",
        "  net_2.eval()\n",
        "\n",
        "  for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state\n",
        "      h = tuple([each.data for each in h])\n",
        "\n",
        "      if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "      # get predicted outputs\n",
        "      output, h = net_2(inputs, h)\n",
        "\n",
        "      # calculate loss\n",
        "      test_loss = criterion(output.squeeze(), labels.float())\n",
        "      test_losses.append(test_loss.item())\n",
        "\n",
        "      # convert output probabilities to predicted class (0 or 1)\n",
        "      pred = torch.round(output.squeeze())\n",
        "\n",
        "      # compare predictions to true label\n",
        "      correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "      correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "      num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "\n",
        "  print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "  # accuracy over all test data\n",
        "  test_acc = num_correct/len(test_loader.dataset)\n",
        "  print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "ukqnubRj8hiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "OZXeZu67hn60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFB9DTg68nc9",
        "outputId": "4d1311bd-fb79-46d8-a17e-3e1a7f71ab1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.778\n",
            "Test accuracy: 0.853\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding dim = 512\n",
        "hidden dim = 256"
      ],
      "metadata": {
        "id": "OArp5-ZY8v6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fANXMein8y-r",
        "outputId": "70f37d07-b0fd-4e35-ade0-5577e357d94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 512)\n",
            "  (lstm): LSTM(512, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "unMuG6vfiRSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF6vh9jv865J",
        "outputId": "409c29ab-4821-42c7-872e-bf4fe4e3607e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.690232... Val Loss: 0.691173 Val Acc: 52.220000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.680128... Val Loss: 0.692224 Val Acc: 51.600000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.688243... Val Loss: 0.686119 Val Acc: 54.820000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.680807... Val Loss: 0.682196 Val Acc: 54.540000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.742827... Val Loss: 0.701086 Val Acc: 50.440000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.701749... Val Loss: 0.692566 Val Acc: 51.560000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.699174... Val Loss: 0.694463 Val Acc: 50.420000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.708339... Val Loss: 0.692744 Val Acc: 49.880000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.702561... Val Loss: 0.691625 Val Acc: 52.600000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.679512... Val Loss: 0.684286 Val Acc: 55.560000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.697731... Val Loss: 0.691431 Val Acc: 56.100000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.645320... Val Loss: 0.659991 Val Acc: 58.640000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.631718... Val Loss: 0.623888 Val Acc: 69.200000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.555389... Val Loss: 0.586688 Val Acc: 72.460000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.604533... Val Loss: 0.600366 Val Acc: 71.500000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.525157... Val Loss: 0.490770 Val Acc: 78.180000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.448889... Val Loss: 0.459414 Val Acc: 79.380000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.450873... Val Loss: 0.447397 Val Acc: 80.320000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.280064... Val Loss: 0.414665 Val Acc: 81.500000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.274387... Val Loss: 0.406879 Val Acc: 82.380000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.361229... Val Loss: 0.400976 Val Acc: 83.700000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.257753... Val Loss: 0.393553 Val Acc: 83.720000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.244078... Val Loss: 0.367054 Val Acc: 84.240000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.400220... Val Loss: 0.358538 Val Acc: 84.620000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.324775... Val Loss: 0.378127 Val Acc: 84.300000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.269542... Val Loss: 0.408016 Val Acc: 84.460000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.371629... Val Loss: 0.426443 Val Acc: 84.560000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.181531... Val Loss: 0.400270 Val Acc: 84.580000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.134064... Val Loss: 0.384779 Val Acc: 85.160000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.134102... Val Loss: 0.370372 Val Acc: 85.480000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.177876... Val Loss: 0.350481 Val Acc: 85.600000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.218129... Val Loss: 0.361084 Val Acc: 85.300000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.070067... Val Loss: 0.411496 Val Acc: 85.760000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.206688... Val Loss: 0.398785 Val Acc: 85.300000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.163388... Val Loss: 0.380799 Val Acc: 85.080000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.133703... Val Loss: 0.373157 Val Acc: 85.800000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.113399... Val Loss: 0.401917 Val Acc: 86.300000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.086491... Val Loss: 0.414610 Val Acc: 86.300000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.377673... Val Loss: 0.479632 Val Acc: 85.560000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.103807... Val Loss: 0.392421 Val Acc: 84.720000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.023799... Val Loss: 0.435150 Val Acc: 85.820000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.091045... Val Loss: 0.483547 Val Acc: 85.580000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.102546... Val Loss: 0.431848 Val Acc: 86.080000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.056985... Val Loss: 0.444688 Val Acc: 85.880000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.147669... Val Loss: 0.440543 Val Acc: 86.180000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.060343... Val Loss: 0.474695 Val Acc: 85.120000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.028005... Val Loss: 0.477000 Val Acc: 85.940000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.120031... Val Loss: 0.495475 Val Acc: 85.960000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.026823... Val Loss: 0.506690 Val Acc: 85.780000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.007345... Val Loss: 0.580354 Val Acc: 86.260000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.007292... Val Loss: 0.591953 Val Acc: 85.820000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.010462... Val Loss: 0.481558 Val Acc: 85.940000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.158092... Val Loss: 0.481949 Val Acc: 86.080000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.015346... Val Loss: 0.530544 Val Acc: 85.640000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.143729... Val Loss: 0.440644 Val Acc: 85.720000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.009298... Val Loss: 0.497222 Val Acc: 85.600000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.112335... Val Loss: 0.576735 Val Acc: 85.400000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.109464... Val Loss: 0.613483 Val Acc: 85.220000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.079386... Val Loss: 0.595296 Val Acc: 85.580000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.044225... Val Loss: 0.594022 Val Acc: 85.340000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.005604... Val Loss: 0.612918 Val Acc: 84.940000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.022335... Val Loss: 0.535539 Val Acc: 85.560000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.122385... Val Loss: 0.593092 Val Acc: 85.200000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.341936... Val Loss: 0.520312 Val Acc: 85.800000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.017620... Val Loss: 0.545317 Val Acc: 85.660000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.002970... Val Loss: 0.640410 Val Acc: 85.460000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.006654... Val Loss: 0.632782 Val Acc: 85.680000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.027319... Val Loss: 0.595480 Val Acc: 85.360000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.019534... Val Loss: 0.571464 Val Acc: 86.420000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.015757... Val Loss: 0.657160 Val Acc: 86.000000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.129025... Val Loss: 0.606226 Val Acc: 85.580000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.015959... Val Loss: 0.582275 Val Acc: 85.140000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.003423... Val Loss: 0.688332 Val Acc: 85.020000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.006721... Val Loss: 0.622332 Val Acc: 85.460000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.011504... Val Loss: 0.601998 Val Acc: 85.260000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.004048... Val Loss: 0.671596 Val Acc: 86.020000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.009341... Val Loss: 0.636089 Val Acc: 85.920000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.036221... Val Loss: 0.604240 Val Acc: 85.000000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.002171... Val Loss: 0.657258 Val Acc: 86.140000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.004494... Val Loss: 0.618418 Val Acc: 85.340000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.007486... Val Loss: 0.658875 Val Acc: 85.680000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.002661... Val Loss: 0.781859 Val Acc: 85.320000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.002037... Val Loss: 0.704991 Val Acc: 85.540000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.001889... Val Loss: 0.724784 Val Acc: 85.420000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.027466... Val Loss: 0.631023 Val Acc: 85.420000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.004712... Val Loss: 0.692889 Val Acc: 85.120000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.004263... Val Loss: 0.666682 Val Acc: 85.520000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.114914... Val Loss: 0.654537 Val Acc: 86.480000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.001954... Val Loss: 0.746087 Val Acc: 85.700000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.002598... Val Loss: 0.743261 Val Acc: 85.420000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.127959... Val Loss: 0.708416 Val Acc: 85.400000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.001963... Val Loss: 0.729060 Val Acc: 84.980000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.001043... Val Loss: 0.771011 Val Acc: 85.260000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.016901... Val Loss: 0.759035 Val Acc: 85.520000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.019432... Val Loss: 0.716435 Val Acc: 85.000000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.041061... Val Loss: 0.704386 Val Acc: 84.580000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.000823... Val Loss: 0.806435 Val Acc: 85.780000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.005011... Val Loss: 0.687159 Val Acc: 84.700000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.157160... Val Loss: 0.696358 Val Acc: 85.400000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.281328... Val Loss: 0.754459 Val Acc: 84.260000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.014884... Val Loss: 0.645708 Val Acc: 85.720000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.002855... Val Loss: 0.716629 Val Acc: 85.600000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.002321... Val Loss: 0.773677 Val Acc: 85.420000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.019872... Val Loss: 0.792043 Val Acc: 85.400000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.000610... Val Loss: 0.790459 Val Acc: 84.640000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.000523... Val Loss: 0.831418 Val Acc: 85.220000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.001512... Val Loss: 0.848622 Val Acc: 84.180000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.005820... Val Loss: 1.011960 Val Acc: 84.380000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.102223... Val Loss: 0.847380 Val Acc: 84.940000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.001178... Val Loss: 0.793657 Val Acc: 85.220000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.000954... Val Loss: 0.913624 Val Acc: 84.260000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.005752... Val Loss: 0.728832 Val Acc: 84.800000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.042188... Val Loss: 0.817701 Val Acc: 84.800000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.057687... Val Loss: 0.788518 Val Acc: 84.740000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.003257... Val Loss: 0.855764 Val Acc: 84.820000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.006375... Val Loss: 0.887613 Val Acc: 84.140000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.009327... Val Loss: 0.733314 Val Acc: 84.260000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.003246... Val Loss: 0.872987 Val Acc: 84.360000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.201830... Val Loss: 0.870392 Val Acc: 85.660000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.024296... Val Loss: 0.781663 Val Acc: 84.600000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "9cktXD3ziTVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xS8UTHlg9AH8",
        "outputId": "754551b5-3d40-4796-8f73-db5524fda0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.834\n",
            "Test accuracy: 0.840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding dim = 128\n",
        "hidden dim = 512"
      ],
      "metadata": {
        "id": "0T7clsOoCSJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1H_8yHjCPbY",
        "outputId": "1de7a3dd-e7b8-44e1-c465-a129dbf3bdcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 128)\n",
            "  (lstm): LSTM(128, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "KmS4ZfEYiYaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_-nsiOdCcgN",
        "outputId": "6034cf60-cd26-4331-fb85-d4837d449205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.703893... Val Loss: 0.695486 Val Acc: 50.580000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.693262... Val Loss: 0.693971 Val Acc: 49.260000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.673590... Val Loss: 0.699349 Val Acc: 49.260000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.691551... Val Loss: 0.693069 Val Acc: 49.780000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.693744... Val Loss: 0.692636 Val Acc: 51.380000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.698308... Val Loss: 0.693778 Val Acc: 49.680000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.690110... Val Loss: 0.691875 Val Acc: 51.840000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.700661... Val Loss: 0.692756 Val Acc: 50.360000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.682380... Val Loss: 0.689403 Val Acc: 52.720000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.680868... Val Loss: 0.716620 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.694885... Val Loss: 0.694184 Val Acc: 50.640000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.692475... Val Loss: 0.693155 Val Acc: 49.920000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.686701... Val Loss: 0.693527 Val Acc: 50.820000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.672586... Val Loss: 0.698609 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.692370... Val Loss: 0.695342 Val Acc: 50.780000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.699553... Val Loss: 0.697534 Val Acc: 50.720000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.697186... Val Loss: 0.693474 Val Acc: 49.300000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.699724... Val Loss: 0.692919 Val Acc: 50.700000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.705211... Val Loss: 0.694937 Val Acc: 49.360000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.695321... Val Loss: 0.694163 Val Acc: 49.280000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.688328... Val Loss: 0.692727 Val Acc: 52.080000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.685907... Val Loss: 0.703756 Val Acc: 49.260000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.692468... Val Loss: 0.693303 Val Acc: 49.280000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.705605... Val Loss: 0.694661 Val Acc: 49.260000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.690880... Val Loss: 0.693002 Val Acc: 50.740000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.696800... Val Loss: 0.693515 Val Acc: 50.780000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.694744... Val Loss: 0.692959 Val Acc: 50.760000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.699041... Val Loss: 0.695632 Val Acc: 49.260000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.694021... Val Loss: 0.693179 Val Acc: 49.260000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.693451... Val Loss: 0.693350 Val Acc: 49.280000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.677751... Val Loss: 0.695169 Val Acc: 49.260000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.692426... Val Loss: 0.693462 Val Acc: 49.260000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.689794... Val Loss: 0.692413 Val Acc: 51.140000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.691508... Val Loss: 0.693127 Val Acc: 49.560000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.704110... Val Loss: 0.693755 Val Acc: 49.440000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.676692... Val Loss: 0.694241 Val Acc: 50.220000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.693530... Val Loss: 0.692927 Val Acc: 49.600000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.672207... Val Loss: 0.684746 Val Acc: 53.500000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.688469... Val Loss: 0.693556 Val Acc: 50.740000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.678635... Val Loss: 0.688306 Val Acc: 53.260000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.625491... Val Loss: 0.660917 Val Acc: 59.260000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.655263... Val Loss: 0.596027 Val Acc: 70.340000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.593583... Val Loss: 0.582558 Val Acc: 70.400000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.441068... Val Loss: 0.517187 Val Acc: 76.360000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.566789... Val Loss: 0.506918 Val Acc: 76.740000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.471022... Val Loss: 0.450926 Val Acc: 79.920000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.390649... Val Loss: 0.454606 Val Acc: 79.960000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.440147... Val Loss: 0.440309 Val Acc: 79.020000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.311545... Val Loss: 0.433483 Val Acc: 81.520000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.203136... Val Loss: 0.430923 Val Acc: 81.940000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.454409... Val Loss: 0.474755 Val Acc: 81.380000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.351111... Val Loss: 0.399544 Val Acc: 82.980000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.372787... Val Loss: 0.414953 Val Acc: 82.540000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.314241... Val Loss: 0.398613 Val Acc: 82.800000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.264233... Val Loss: 0.382686 Val Acc: 83.880000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.349425... Val Loss: 0.365101 Val Acc: 84.120000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.257463... Val Loss: 0.396395 Val Acc: 83.540000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.245864... Val Loss: 0.384154 Val Acc: 84.340000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.205804... Val Loss: 0.374650 Val Acc: 84.660000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.344894... Val Loss: 0.364047 Val Acc: 84.060000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.191048... Val Loss: 0.369898 Val Acc: 84.880000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.157387... Val Loss: 0.355227 Val Acc: 85.600000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.171798... Val Loss: 0.373054 Val Acc: 85.420000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.425323... Val Loss: 0.371332 Val Acc: 85.880000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.214495... Val Loss: 0.379932 Val Acc: 85.460000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.190002... Val Loss: 0.380666 Val Acc: 85.640000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.249505... Val Loss: 0.377355 Val Acc: 85.500000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.234744... Val Loss: 0.379350 Val Acc: 85.540000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.077179... Val Loss: 0.393992 Val Acc: 85.220000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.229976... Val Loss: 0.406044 Val Acc: 84.440000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.282894... Val Loss: 0.382595 Val Acc: 85.500000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.286096... Val Loss: 0.370276 Val Acc: 86.380000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.089531... Val Loss: 0.412442 Val Acc: 85.560000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.157497... Val Loss: 0.412242 Val Acc: 85.540000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.202142... Val Loss: 0.408766 Val Acc: 85.680000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.124940... Val Loss: 0.409456 Val Acc: 85.660000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.083453... Val Loss: 0.394914 Val Acc: 84.700000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.084933... Val Loss: 0.407876 Val Acc: 86.320000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.191889... Val Loss: 0.430669 Val Acc: 84.680000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.129827... Val Loss: 0.398306 Val Acc: 85.060000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.033525... Val Loss: 0.451623 Val Acc: 86.040000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.058923... Val Loss: 0.443582 Val Acc: 85.440000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.023267... Val Loss: 0.422939 Val Acc: 85.660000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.068417... Val Loss: 0.457484 Val Acc: 86.360000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.214650... Val Loss: 0.474826 Val Acc: 85.080000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.132571... Val Loss: 0.475859 Val Acc: 84.060000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.148017... Val Loss: 0.421458 Val Acc: 85.680000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.043784... Val Loss: 0.436411 Val Acc: 86.040000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.030423... Val Loss: 0.497267 Val Acc: 85.160000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.211214... Val Loss: 0.503119 Val Acc: 86.120000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.129550... Val Loss: 0.536234 Val Acc: 85.520000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.010789... Val Loss: 0.520344 Val Acc: 86.200000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.137050... Val Loss: 0.520601 Val Acc: 85.280000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.035566... Val Loss: 0.491997 Val Acc: 83.980000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.044363... Val Loss: 0.597404 Val Acc: 84.200000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.024550... Val Loss: 0.513269 Val Acc: 85.680000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.111726... Val Loss: 0.534407 Val Acc: 85.740000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.010921... Val Loss: 0.558471 Val Acc: 85.460000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.007939... Val Loss: 0.518265 Val Acc: 85.600000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.013815... Val Loss: 0.552388 Val Acc: 85.880000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.137292... Val Loss: 0.615010 Val Acc: 84.980000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.184645... Val Loss: 0.576564 Val Acc: 85.480000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.039279... Val Loss: 0.529267 Val Acc: 85.680000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.006584... Val Loss: 0.523998 Val Acc: 85.780000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.002198... Val Loss: 0.641508 Val Acc: 86.300000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.012413... Val Loss: 0.561200 Val Acc: 84.600000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.024384... Val Loss: 0.565184 Val Acc: 85.660000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.014177... Val Loss: 0.617127 Val Acc: 85.300000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.116560... Val Loss: 0.605701 Val Acc: 85.100000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.035452... Val Loss: 0.579411 Val Acc: 84.760000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.055220... Val Loss: 0.525056 Val Acc: 85.140000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.013655... Val Loss: 0.578388 Val Acc: 85.100000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.035282... Val Loss: 0.630352 Val Acc: 85.140000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.024228... Val Loss: 0.562269 Val Acc: 86.180000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.009849... Val Loss: 0.577437 Val Acc: 85.540000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.005918... Val Loss: 0.581724 Val Acc: 86.160000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.006353... Val Loss: 0.602144 Val Acc: 85.340000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.021068... Val Loss: 0.587580 Val Acc: 85.380000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.017994... Val Loss: 0.554721 Val Acc: 85.360000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.005006... Val Loss: 0.690241 Val Acc: 84.880000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding dim = 256\n",
        "hidden dim = 512"
      ],
      "metadata": {
        "id": "EXYLOp_dIn7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0jNpX1fIr2H",
        "outputId": "39df4a6d-8638-45f3-91c4-4f1e9e4e8806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 256)\n",
            "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "UefvKGtCijxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa43Skc9Isi6",
        "outputId": "2bc4124b-d5d0-4727-d94e-54f75e4b7317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.708381... Val Loss: 0.691819 Val Acc: 52.360000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.694353... Val Loss: 0.691338 Val Acc: 51.900000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.721463... Val Loss: 0.699789 Val Acc: 50.140000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.702218... Val Loss: 0.699755 Val Acc: 50.020000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.711733... Val Loss: 0.695870 Val Acc: 50.740000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.691102... Val Loss: 0.693204 Val Acc: 50.100000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.674386... Val Loss: 0.695389 Val Acc: 49.840000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.697794... Val Loss: 0.693075 Val Acc: 50.480000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.695721... Val Loss: 0.693128 Val Acc: 50.140000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.694018... Val Loss: 0.694723 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.692125... Val Loss: 0.693200 Val Acc: 50.080000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.695096... Val Loss: 0.693021 Val Acc: 50.740000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.693295... Val Loss: 0.693394 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.695425... Val Loss: 0.693520 Val Acc: 49.260000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.696753... Val Loss: 0.693113 Val Acc: 50.700000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.694917... Val Loss: 0.693032 Val Acc: 50.720000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.695733... Val Loss: 0.694264 Val Acc: 49.260000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.694408... Val Loss: 0.693064 Val Acc: 50.660000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.695238... Val Loss: 0.694259 Val Acc: 49.260000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.695129... Val Loss: 0.693011 Val Acc: 50.660000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.693993... Val Loss: 0.692676 Val Acc: 51.360000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.698155... Val Loss: 0.692634 Val Acc: 50.220000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.692275... Val Loss: 0.693357 Val Acc: 50.700000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.688939... Val Loss: 0.691036 Val Acc: 52.340000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.694185... Val Loss: 0.692985 Val Acc: 50.760000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.695422... Val Loss: 0.693320 Val Acc: 50.040000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.689602... Val Loss: 0.692562 Val Acc: 51.120000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.694857... Val Loss: 0.690793 Val Acc: 52.020000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.698971... Val Loss: 0.685705 Val Acc: 53.640000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.689080... Val Loss: 0.691430 Val Acc: 52.180000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.664466... Val Loss: 0.657068 Val Acc: 65.860000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.714801... Val Loss: 0.635227 Val Acc: 68.220000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.529045... Val Loss: 0.544188 Val Acc: 74.220000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.624955... Val Loss: 0.515369 Val Acc: 75.700000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.566613... Val Loss: 0.461613 Val Acc: 79.160000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.450053... Val Loss: 0.431055 Val Acc: 80.420000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.475760... Val Loss: 0.505398 Val Acc: 76.800000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.391098... Val Loss: 0.404301 Val Acc: 81.880000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.257081... Val Loss: 0.387532 Val Acc: 83.540000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.463868... Val Loss: 0.395672 Val Acc: 82.660000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.173375... Val Loss: 0.377198 Val Acc: 84.440000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.241803... Val Loss: 0.376288 Val Acc: 83.840000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.390096... Val Loss: 0.424603 Val Acc: 80.380000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.309660... Val Loss: 0.424946 Val Acc: 79.300000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.424255... Val Loss: 0.374237 Val Acc: 82.620000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.539610... Val Loss: 0.356018 Val Acc: 84.900000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.359925... Val Loss: 0.351168 Val Acc: 85.120000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.399727... Val Loss: 0.403497 Val Acc: 82.320000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.360176... Val Loss: 0.416707 Val Acc: 77.840000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.369018... Val Loss: 0.423607 Val Acc: 80.260000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.417407... Val Loss: 0.415761 Val Acc: 79.760000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.588902... Val Loss: 0.650310 Val Acc: 56.620000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.689687... Val Loss: 0.670493 Val Acc: 53.620000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.664371... Val Loss: 0.666843 Val Acc: 54.720000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.643565... Val Loss: 0.670881 Val Acc: 55.960000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.728121... Val Loss: 0.644342 Val Acc: 58.660000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.699373... Val Loss: 0.741091 Val Acc: 55.060000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.381354... Val Loss: 0.478245 Val Acc: 79.720000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.371257... Val Loss: 0.543636 Val Acc: 77.940000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.418143... Val Loss: 0.552689 Val Acc: 75.420000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.318883... Val Loss: 0.451179 Val Acc: 80.400000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.307009... Val Loss: 0.440236 Val Acc: 81.860000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.374493... Val Loss: 0.446416 Val Acc: 81.700000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.242878... Val Loss: 0.427454 Val Acc: 82.820000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.256249... Val Loss: 0.442235 Val Acc: 82.720000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.227829... Val Loss: 0.402447 Val Acc: 83.400000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.291600... Val Loss: 0.403918 Val Acc: 84.140000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.357034... Val Loss: 0.394081 Val Acc: 84.020000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.224751... Val Loss: 0.395583 Val Acc: 84.720000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.422577... Val Loss: 0.476583 Val Acc: 81.140000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.173340... Val Loss: 0.382801 Val Acc: 84.420000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.337105... Val Loss: 0.383632 Val Acc: 83.880000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.102712... Val Loss: 0.485919 Val Acc: 81.620000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.190167... Val Loss: 0.408714 Val Acc: 84.360000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.199457... Val Loss: 0.436677 Val Acc: 85.920000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.165783... Val Loss: 0.399808 Val Acc: 85.480000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.276658... Val Loss: 0.468593 Val Acc: 83.040000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.146847... Val Loss: 0.428086 Val Acc: 84.240000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.397642... Val Loss: 0.413684 Val Acc: 82.760000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.215656... Val Loss: 0.386471 Val Acc: 83.640000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.234765... Val Loss: 0.399204 Val Acc: 84.100000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.161277... Val Loss: 0.443389 Val Acc: 84.940000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.098043... Val Loss: 0.424649 Val Acc: 85.840000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.062796... Val Loss: 0.411493 Val Acc: 85.880000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.114146... Val Loss: 0.427411 Val Acc: 86.260000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.094560... Val Loss: 0.407881 Val Acc: 86.040000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.065305... Val Loss: 0.420588 Val Acc: 83.700000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.054203... Val Loss: 0.427963 Val Acc: 84.540000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.052957... Val Loss: 0.439499 Val Acc: 86.020000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.113453... Val Loss: 0.423421 Val Acc: 86.300000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.169173... Val Loss: 0.413151 Val Acc: 86.240000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.314798... Val Loss: 0.465809 Val Acc: 86.180000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.073709... Val Loss: 0.422795 Val Acc: 84.860000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.262430... Val Loss: 0.472128 Val Acc: 84.380000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.060287... Val Loss: 0.410806 Val Acc: 85.820000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.111893... Val Loss: 0.411249 Val Acc: 85.900000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.042252... Val Loss: 0.469312 Val Acc: 85.820000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.075011... Val Loss: 0.462302 Val Acc: 85.440000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.085605... Val Loss: 0.446041 Val Acc: 86.080000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.154385... Val Loss: 0.473526 Val Acc: 85.860000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.226991... Val Loss: 0.467098 Val Acc: 85.700000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.035715... Val Loss: 0.459892 Val Acc: 86.620000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.172842... Val Loss: 0.484344 Val Acc: 85.560000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.222290... Val Loss: 0.434390 Val Acc: 85.900000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.075111... Val Loss: 0.523795 Val Acc: 85.640000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.010035... Val Loss: 0.505326 Val Acc: 85.760000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.005320... Val Loss: 0.512614 Val Acc: 85.740000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.012503... Val Loss: 0.534141 Val Acc: 86.100000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.028465... Val Loss: 0.524437 Val Acc: 85.500000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.036829... Val Loss: 0.522124 Val Acc: 85.120000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.058284... Val Loss: 0.500230 Val Acc: 85.880000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.073001... Val Loss: 0.534081 Val Acc: 84.760000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.007696... Val Loss: 0.588725 Val Acc: 86.040000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.012866... Val Loss: 0.586797 Val Acc: 85.420000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.011836... Val Loss: 0.720895 Val Acc: 84.060000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.041022... Val Loss: 0.615402 Val Acc: 85.320000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.122502... Val Loss: 0.609510 Val Acc: 84.900000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.008028... Val Loss: 0.681969 Val Acc: 85.240000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.008193... Val Loss: 0.554368 Val Acc: 84.540000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.008380... Val Loss: 0.650492 Val Acc: 84.640000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Network Architecture**\n",
        "\n",
        "embedding dim = 512\n",
        "hidden dim = 512"
      ],
      "metadata": {
        "id": "POSq-ssyPhjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juW5cO8kPl9w",
        "outputId": "1d37dea9-bfb5-4ca6-db34-7c1ad235be05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 512)\n",
            "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "arN8T61hiqcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net_2 = train_lstm(net_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYoaCMdVPnvR",
        "outputId": "b14b2129-bb89-443a-dccd-9d6181ba0d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/15... Step: 100... Loss: 0.690307... Val Loss: 0.692677 Val Acc: 50.420000\n",
            "Epoch: 1/15... Step: 200... Loss: 0.678484... Val Loss: 0.687587 Val Acc: 53.400000\n",
            "Epoch: 1/15... Step: 300... Loss: 0.699270... Val Loss: 0.682307 Val Acc: 59.500000\n",
            "Epoch: 1/15... Step: 400... Loss: 0.698131... Val Loss: 0.690434 Val Acc: 53.860000\n",
            "Epoch: 1/15... Step: 500... Loss: 0.698116... Val Loss: 0.681058 Val Acc: 54.800000\n",
            "Epoch: 1/15... Step: 600... Loss: 0.645769... Val Loss: 0.658957 Val Acc: 58.780000\n",
            "Epoch: 1/15... Step: 700... Loss: 0.458977... Val Loss: 0.606260 Val Acc: 71.000000\n",
            "Epoch: 1/15... Step: 800... Loss: 0.501150... Val Loss: 0.499541 Val Acc: 77.200000\n",
            "Epoch: 2/15... Step: 900... Loss: 0.468834... Val Loss: 0.490180 Val Acc: 75.700000\n",
            "Epoch: 2/15... Step: 1000... Loss: 0.491324... Val Loss: 0.442329 Val Acc: 81.300000\n",
            "Epoch: 2/15... Step: 1100... Loss: 0.341583... Val Loss: 0.449092 Val Acc: 80.260000\n",
            "Epoch: 2/15... Step: 1200... Loss: 0.271244... Val Loss: 0.440328 Val Acc: 79.460000\n",
            "Epoch: 2/15... Step: 1300... Loss: 0.239543... Val Loss: 0.389867 Val Acc: 83.420000\n",
            "Epoch: 2/15... Step: 1400... Loss: 0.536593... Val Loss: 0.468466 Val Acc: 81.540000\n",
            "Epoch: 2/15... Step: 1500... Loss: 0.244716... Val Loss: 0.361716 Val Acc: 84.440000\n",
            "Epoch: 2/15... Step: 1600... Loss: 0.335018... Val Loss: 0.378626 Val Acc: 83.740000\n",
            "Epoch: 3/15... Step: 1700... Loss: 0.313317... Val Loss: 0.384098 Val Acc: 82.880000\n",
            "Epoch: 3/15... Step: 1800... Loss: 0.217232... Val Loss: 0.390697 Val Acc: 84.520000\n",
            "Epoch: 3/15... Step: 1900... Loss: 0.292079... Val Loss: 0.352021 Val Acc: 84.340000\n",
            "Epoch: 3/15... Step: 2000... Loss: 0.371399... Val Loss: 0.357043 Val Acc: 84.440000\n",
            "Epoch: 3/15... Step: 2100... Loss: 0.198636... Val Loss: 0.366251 Val Acc: 85.320000\n",
            "Epoch: 3/15... Step: 2200... Loss: 0.280067... Val Loss: 0.345928 Val Acc: 85.780000\n",
            "Epoch: 3/15... Step: 2300... Loss: 0.180092... Val Loss: 0.356880 Val Acc: 85.480000\n",
            "Epoch: 3/15... Step: 2400... Loss: 0.253889... Val Loss: 0.344115 Val Acc: 85.140000\n",
            "Epoch: 4/15... Step: 2500... Loss: 0.344788... Val Loss: 0.402691 Val Acc: 84.840000\n",
            "Epoch: 4/15... Step: 2600... Loss: 0.133141... Val Loss: 0.403842 Val Acc: 85.460000\n",
            "Epoch: 4/15... Step: 2700... Loss: 0.143625... Val Loss: 0.388993 Val Acc: 84.480000\n",
            "Epoch: 4/15... Step: 2800... Loss: 0.226775... Val Loss: 0.375522 Val Acc: 85.980000\n",
            "Epoch: 4/15... Step: 2900... Loss: 0.148603... Val Loss: 0.380982 Val Acc: 85.960000\n",
            "Epoch: 4/15... Step: 3000... Loss: 0.120967... Val Loss: 0.372124 Val Acc: 86.320000\n",
            "Epoch: 4/15... Step: 3100... Loss: 0.260935... Val Loss: 0.389726 Val Acc: 85.120000\n",
            "Epoch: 4/15... Step: 3200... Loss: 0.101274... Val Loss: 0.375456 Val Acc: 86.400000\n",
            "Epoch: 5/15... Step: 3300... Loss: 0.057585... Val Loss: 0.460912 Val Acc: 85.860000\n",
            "Epoch: 5/15... Step: 3400... Loss: 0.019379... Val Loss: 0.465111 Val Acc: 86.440000\n",
            "Epoch: 5/15... Step: 3500... Loss: 0.110136... Val Loss: 0.437883 Val Acc: 86.300000\n",
            "Epoch: 5/15... Step: 3600... Loss: 0.102020... Val Loss: 0.425269 Val Acc: 85.400000\n",
            "Epoch: 5/15... Step: 3700... Loss: 0.018987... Val Loss: 0.445962 Val Acc: 85.800000\n",
            "Epoch: 5/15... Step: 3800... Loss: 0.194597... Val Loss: 0.393140 Val Acc: 85.980000\n",
            "Epoch: 5/15... Step: 3900... Loss: 0.126669... Val Loss: 0.377551 Val Acc: 86.560000\n",
            "Epoch: 5/15... Step: 4000... Loss: 0.081720... Val Loss: 0.392305 Val Acc: 86.700000\n",
            "Epoch: 6/15... Step: 4100... Loss: 0.010575... Val Loss: 0.501882 Val Acc: 86.040000\n",
            "Epoch: 6/15... Step: 4200... Loss: 0.150910... Val Loss: 0.554075 Val Acc: 85.900000\n",
            "Epoch: 6/15... Step: 4300... Loss: 0.015301... Val Loss: 0.480662 Val Acc: 86.400000\n",
            "Epoch: 6/15... Step: 4400... Loss: 0.227554... Val Loss: 0.448976 Val Acc: 84.800000\n",
            "Epoch: 6/15... Step: 4500... Loss: 0.042950... Val Loss: 0.467095 Val Acc: 85.840000\n",
            "Epoch: 6/15... Step: 4600... Loss: 0.091440... Val Loss: 0.517973 Val Acc: 85.720000\n",
            "Epoch: 6/15... Step: 4700... Loss: 0.023069... Val Loss: 0.495908 Val Acc: 85.580000\n",
            "Epoch: 6/15... Step: 4800... Loss: 0.036269... Val Loss: 0.509247 Val Acc: 86.240000\n",
            "Epoch: 7/15... Step: 4900... Loss: 0.005472... Val Loss: 0.672096 Val Acc: 85.940000\n",
            "Epoch: 7/15... Step: 5000... Loss: 0.108376... Val Loss: 0.546823 Val Acc: 85.880000\n",
            "Epoch: 7/15... Step: 5100... Loss: 0.027823... Val Loss: 0.534435 Val Acc: 86.000000\n",
            "Epoch: 7/15... Step: 5200... Loss: 0.136179... Val Loss: 0.560703 Val Acc: 86.240000\n",
            "Epoch: 7/15... Step: 5300... Loss: 0.025051... Val Loss: 0.541893 Val Acc: 86.480000\n",
            "Epoch: 7/15... Step: 5400... Loss: 0.028765... Val Loss: 0.576102 Val Acc: 86.700000\n",
            "Epoch: 7/15... Step: 5500... Loss: 0.014668... Val Loss: 0.532591 Val Acc: 86.420000\n",
            "Epoch: 7/15... Step: 5600... Loss: 0.062620... Val Loss: 0.585368 Val Acc: 86.280000\n",
            "Epoch: 8/15... Step: 5700... Loss: 0.011422... Val Loss: 0.622949 Val Acc: 86.740000\n",
            "Epoch: 8/15... Step: 5800... Loss: 0.039366... Val Loss: 0.601586 Val Acc: 85.860000\n",
            "Epoch: 8/15... Step: 5900... Loss: 0.005465... Val Loss: 0.570090 Val Acc: 86.240000\n",
            "Epoch: 8/15... Step: 6000... Loss: 0.007765... Val Loss: 0.595381 Val Acc: 85.260000\n",
            "Epoch: 8/15... Step: 6100... Loss: 0.077119... Val Loss: 0.628961 Val Acc: 85.820000\n",
            "Epoch: 8/15... Step: 6200... Loss: 0.008453... Val Loss: 0.617431 Val Acc: 85.860000\n",
            "Epoch: 8/15... Step: 6300... Loss: 0.013815... Val Loss: 0.603781 Val Acc: 85.140000\n",
            "Epoch: 8/15... Step: 6400... Loss: 0.010703... Val Loss: 0.577986 Val Acc: 84.860000\n",
            "Epoch: 9/15... Step: 6500... Loss: 0.001716... Val Loss: 0.646159 Val Acc: 85.380000\n",
            "Epoch: 9/15... Step: 6600... Loss: 0.032102... Val Loss: 0.603294 Val Acc: 84.980000\n",
            "Epoch: 9/15... Step: 6700... Loss: 0.134103... Val Loss: 0.651277 Val Acc: 86.000000\n",
            "Epoch: 9/15... Step: 6800... Loss: 0.135212... Val Loss: 0.585461 Val Acc: 85.120000\n",
            "Epoch: 9/15... Step: 6900... Loss: 0.006575... Val Loss: 0.704264 Val Acc: 85.120000\n",
            "Epoch: 9/15... Step: 7000... Loss: 0.004929... Val Loss: 0.806382 Val Acc: 84.420000\n",
            "Epoch: 9/15... Step: 7100... Loss: 0.021701... Val Loss: 0.632447 Val Acc: 85.320000\n",
            "Epoch: 9/15... Step: 7200... Loss: 0.001834... Val Loss: 0.638150 Val Acc: 85.460000\n",
            "Epoch: 10/15... Step: 7300... Loss: 0.002154... Val Loss: 0.744208 Val Acc: 84.800000\n",
            "Epoch: 10/15... Step: 7400... Loss: 0.001646... Val Loss: 0.727004 Val Acc: 85.800000\n",
            "Epoch: 10/15... Step: 7500... Loss: 0.002073... Val Loss: 0.663320 Val Acc: 85.580000\n",
            "Epoch: 10/15... Step: 7600... Loss: 0.001445... Val Loss: 0.774214 Val Acc: 86.340000\n",
            "Epoch: 10/15... Step: 7700... Loss: 0.005018... Val Loss: 0.759988 Val Acc: 85.660000\n",
            "Epoch: 10/15... Step: 7800... Loss: 0.002789... Val Loss: 0.782152 Val Acc: 86.260000\n",
            "Epoch: 10/15... Step: 7900... Loss: 0.004213... Val Loss: 0.733738 Val Acc: 85.140000\n",
            "Epoch: 10/15... Step: 8000... Loss: 0.003444... Val Loss: 0.644763 Val Acc: 85.520000\n",
            "Epoch: 11/15... Step: 8100... Loss: 0.000561... Val Loss: 0.825987 Val Acc: 85.780000\n",
            "Epoch: 11/15... Step: 8200... Loss: 0.024867... Val Loss: 0.677713 Val Acc: 85.860000\n",
            "Epoch: 11/15... Step: 8300... Loss: 0.001303... Val Loss: 0.703411 Val Acc: 86.180000\n",
            "Epoch: 11/15... Step: 8400... Loss: 0.019211... Val Loss: 0.700217 Val Acc: 85.980000\n",
            "Epoch: 11/15... Step: 8500... Loss: 0.013570... Val Loss: 0.788614 Val Acc: 85.080000\n",
            "Epoch: 11/15... Step: 8600... Loss: 0.109624... Val Loss: 0.740330 Val Acc: 85.660000\n",
            "Epoch: 11/15... Step: 8700... Loss: 0.061211... Val Loss: 0.726089 Val Acc: 86.360000\n",
            "Epoch: 11/15... Step: 8800... Loss: 0.034675... Val Loss: 0.771652 Val Acc: 85.100000\n",
            "Epoch: 12/15... Step: 8900... Loss: 0.098109... Val Loss: 0.845700 Val Acc: 86.000000\n",
            "Epoch: 12/15... Step: 9000... Loss: 0.003087... Val Loss: 0.742051 Val Acc: 85.520000\n",
            "Epoch: 12/15... Step: 9100... Loss: 0.001183... Val Loss: 0.762853 Val Acc: 86.600000\n",
            "Epoch: 12/15... Step: 9200... Loss: 0.002486... Val Loss: 0.884113 Val Acc: 85.860000\n",
            "Epoch: 12/15... Step: 9300... Loss: 0.009631... Val Loss: 0.758506 Val Acc: 86.020000\n",
            "Epoch: 12/15... Step: 9400... Loss: 0.006257... Val Loss: 0.758355 Val Acc: 85.320000\n",
            "Epoch: 12/15... Step: 9500... Loss: 0.001381... Val Loss: 0.722289 Val Acc: 85.640000\n",
            "Epoch: 12/15... Step: 9600... Loss: 0.002533... Val Loss: 0.721905 Val Acc: 84.800000\n",
            "Epoch: 13/15... Step: 9700... Loss: 0.031167... Val Loss: 0.852180 Val Acc: 85.400000\n",
            "Epoch: 13/15... Step: 9800... Loss: 0.001432... Val Loss: 0.849789 Val Acc: 85.980000\n",
            "Epoch: 13/15... Step: 9900... Loss: 0.005640... Val Loss: 0.839536 Val Acc: 85.420000\n",
            "Epoch: 13/15... Step: 10000... Loss: 0.073144... Val Loss: 0.847202 Val Acc: 85.280000\n",
            "Epoch: 13/15... Step: 10100... Loss: 0.006806... Val Loss: 0.802812 Val Acc: 84.460000\n",
            "Epoch: 13/15... Step: 10200... Loss: 0.003377... Val Loss: 0.775737 Val Acc: 85.560000\n",
            "Epoch: 13/15... Step: 10300... Loss: 0.004141... Val Loss: 0.725189 Val Acc: 86.060000\n",
            "Epoch: 13/15... Step: 10400... Loss: 0.003348... Val Loss: 0.784196 Val Acc: 86.020000\n",
            "Epoch: 14/15... Step: 10500... Loss: 0.073788... Val Loss: 0.746525 Val Acc: 85.100000\n",
            "Epoch: 14/15... Step: 10600... Loss: 0.023324... Val Loss: 0.788525 Val Acc: 86.580000\n",
            "Epoch: 14/15... Step: 10700... Loss: 0.006527... Val Loss: 0.827852 Val Acc: 85.380000\n",
            "Epoch: 14/15... Step: 10800... Loss: 0.019683... Val Loss: 0.850078 Val Acc: 85.480000\n",
            "Epoch: 14/15... Step: 10900... Loss: 0.001831... Val Loss: 0.777738 Val Acc: 85.240000\n",
            "Epoch: 14/15... Step: 11000... Loss: 0.005870... Val Loss: 0.790695 Val Acc: 85.560000\n",
            "Epoch: 14/15... Step: 11100... Loss: 0.006783... Val Loss: 0.800987 Val Acc: 84.900000\n",
            "Epoch: 14/15... Step: 11200... Loss: 0.000506... Val Loss: 0.867531 Val Acc: 85.500000\n",
            "Epoch: 15/15... Step: 11300... Loss: 0.000795... Val Loss: 0.930028 Val Acc: 85.320000\n",
            "Epoch: 15/15... Step: 11400... Loss: 0.000192... Val Loss: 0.927103 Val Acc: 85.500000\n",
            "Epoch: 15/15... Step: 11500... Loss: 0.100996... Val Loss: 0.966895 Val Acc: 85.400000\n",
            "Epoch: 15/15... Step: 11600... Loss: 0.002093... Val Loss: 1.054473 Val Acc: 83.900000\n",
            "Epoch: 15/15... Step: 11700... Loss: 0.001209... Val Loss: 0.890643 Val Acc: 85.100000\n",
            "Epoch: 15/15... Step: 11800... Loss: 0.000945... Val Loss: 0.907423 Val Acc: 85.380000\n",
            "Epoch: 15/15... Step: 11900... Loss: 0.000257... Val Loss: 0.929224 Val Acc: 85.080000\n",
            "Epoch: 15/15... Step: 12000... Loss: 0.008223... Val Loss: 0.880689 Val Acc: 84.960000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### train model for predictions\n"
      ],
      "metadata": {
        "id": "UtrmGYlkYONZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYjLcZHSYZCM",
        "outputId": "91966c65-bdd8-4411-8386-72e653c7de94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 512)\n",
            "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip = 5\n",
        "\n",
        "if train_on_gpu:\n",
        "    net_2.cuda()\n",
        "\n",
        "net_2.train()\n",
        "best_val_acc = 0\n",
        "epoch_losses = []\n",
        "\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net_2.init_hidden(batch_size)\n",
        "\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        if train_on_gpu:\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "        # Creating new variables for the hidden state\n",
        "        h = tuple([each.data for each in h])\n",
        "\n",
        "\n",
        "        output, h = net_2(inputs, h)\n",
        "\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        net_2.zero_grad()\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        nn.utils.clip_grad_norm_(net_2.parameters(), clip)\n",
        "\n",
        "        # Update weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "\n",
        "            val_h = net_2.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            net_2.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "                if train_on_gpu:\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                output, val_h = net_2(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "                predicted = (output.squeeze() >= 0.5).float()\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "            val_acc = 100 * correct / total\n",
        "            epoch_losses.append(val_acc)\n",
        "            net_2.train()\n",
        "            print(\"Epoch: {}/{}...\".format(e + 1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)),\n",
        "                  \"Val Acc: {:.6f}\".format(val_acc))\n",
        "            if val_acc > best_val_acc:\n",
        "              best_val_acc = val_acc\n",
        "\n",
        "              # Save the model\n",
        "              checkpoint = {\n",
        "                  'state_dict': net_2.state_dict(),\n",
        "                  'best_val_acc': best_val_acc,\n",
        "                  'optimizer': optimizer.state_dict(),\n",
        "                  'epoch': e\n",
        "              }\n",
        "              torch.save(checkpoint, 'best_model.pth')\n",
        "\n",
        "              print(\"Model saved with higher validation accuracy: {:.6f}\".format(best_val_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5dzCTUUYRnR",
        "outputId": "e950602f-5eba-4730-d3a9-96c3c33931ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 100... Loss: 0.688842... Val Loss: 0.689447 Val Acc: 52.400000\n",
            "Model saved with higher validation accuracy: 52.400000\n",
            "Epoch: 1/20... Step: 200... Loss: 0.663496... Val Loss: 0.727650 Val Acc: 50.820000\n",
            "Epoch: 1/20... Step: 300... Loss: 0.694687... Val Loss: 0.689579 Val Acc: 53.040000\n",
            "Model saved with higher validation accuracy: 53.040000\n",
            "Epoch: 1/20... Step: 400... Loss: 0.705066... Val Loss: 0.697330 Val Acc: 49.260000\n",
            "Epoch: 1/20... Step: 500... Loss: 0.684458... Val Loss: 0.692753 Val Acc: 50.780000\n",
            "Epoch: 1/20... Step: 600... Loss: 0.695380... Val Loss: 0.676613 Val Acc: 61.620000\n",
            "Model saved with higher validation accuracy: 61.620000\n",
            "Epoch: 1/20... Step: 700... Loss: 0.688416... Val Loss: 0.694151 Val Acc: 49.280000\n",
            "Epoch: 1/20... Step: 800... Loss: 0.690874... Val Loss: 0.693174 Val Acc: 49.340000\n",
            "Epoch: 2/20... Step: 900... Loss: 0.681359... Val Loss: 0.691504 Val Acc: 51.140000\n",
            "Epoch: 2/20... Step: 1000... Loss: 0.625029... Val Loss: 0.635315 Val Acc: 69.200000\n",
            "Model saved with higher validation accuracy: 69.200000\n",
            "Epoch: 2/20... Step: 1100... Loss: 0.688875... Val Loss: 0.689361 Val Acc: 52.220000\n",
            "Epoch: 2/20... Step: 1200... Loss: 0.716828... Val Loss: 0.677088 Val Acc: 60.980000\n",
            "Epoch: 2/20... Step: 1300... Loss: 0.652332... Val Loss: 0.555890 Val Acc: 74.760000\n",
            "Model saved with higher validation accuracy: 74.760000\n",
            "Epoch: 2/20... Step: 1400... Loss: 0.515319... Val Loss: 0.463645 Val Acc: 78.700000\n",
            "Model saved with higher validation accuracy: 78.700000\n",
            "Epoch: 2/20... Step: 1500... Loss: 0.458811... Val Loss: 0.452611 Val Acc: 80.480000\n",
            "Model saved with higher validation accuracy: 80.480000\n",
            "Epoch: 2/20... Step: 1600... Loss: 0.476093... Val Loss: 0.415587 Val Acc: 82.300000\n",
            "Model saved with higher validation accuracy: 82.300000\n",
            "Epoch: 3/20... Step: 1700... Loss: 0.242601... Val Loss: 0.418192 Val Acc: 83.260000\n",
            "Model saved with higher validation accuracy: 83.260000\n",
            "Epoch: 3/20... Step: 1800... Loss: 0.323088... Val Loss: 0.401723 Val Acc: 82.520000\n",
            "Epoch: 3/20... Step: 1900... Loss: 0.254481... Val Loss: 0.382496 Val Acc: 83.780000\n",
            "Model saved with higher validation accuracy: 83.780000\n",
            "Epoch: 3/20... Step: 2000... Loss: 0.288652... Val Loss: 0.363260 Val Acc: 84.360000\n",
            "Model saved with higher validation accuracy: 84.360000\n",
            "Epoch: 3/20... Step: 2100... Loss: 0.248900... Val Loss: 0.365161 Val Acc: 83.680000\n",
            "Epoch: 3/20... Step: 2200... Loss: 0.244689... Val Loss: 0.360394 Val Acc: 84.820000\n",
            "Model saved with higher validation accuracy: 84.820000\n",
            "Epoch: 3/20... Step: 2300... Loss: 0.334850... Val Loss: 0.342418 Val Acc: 85.540000\n",
            "Model saved with higher validation accuracy: 85.540000\n",
            "Epoch: 3/20... Step: 2400... Loss: 0.348965... Val Loss: 0.363968 Val Acc: 84.400000\n",
            "Epoch: 4/20... Step: 2500... Loss: 0.146055... Val Loss: 0.364471 Val Acc: 85.360000\n",
            "Epoch: 4/20... Step: 2600... Loss: 0.193326... Val Loss: 0.353170 Val Acc: 85.120000\n",
            "Epoch: 4/20... Step: 2700... Loss: 0.161766... Val Loss: 0.361130 Val Acc: 85.880000\n",
            "Model saved with higher validation accuracy: 85.880000\n",
            "Epoch: 4/20... Step: 2800... Loss: 0.220198... Val Loss: 0.371509 Val Acc: 86.380000\n",
            "Model saved with higher validation accuracy: 86.380000\n",
            "Epoch: 4/20... Step: 2900... Loss: 0.184644... Val Loss: 0.403725 Val Acc: 85.260000\n",
            "Epoch: 4/20... Step: 3000... Loss: 0.395952... Val Loss: 0.352726 Val Acc: 85.260000\n",
            "Epoch: 4/20... Step: 3100... Loss: 0.252736... Val Loss: 0.372913 Val Acc: 86.240000\n",
            "Epoch: 4/20... Step: 3200... Loss: 0.301270... Val Loss: 0.360579 Val Acc: 86.200000\n",
            "Epoch: 5/20... Step: 3300... Loss: 0.068106... Val Loss: 0.420513 Val Acc: 85.220000\n",
            "Epoch: 5/20... Step: 3400... Loss: 0.179282... Val Loss: 0.413747 Val Acc: 85.400000\n",
            "Epoch: 5/20... Step: 3500... Loss: 0.146698... Val Loss: 0.441224 Val Acc: 85.280000\n",
            "Epoch: 5/20... Step: 3600... Loss: 0.146163... Val Loss: 0.466259 Val Acc: 84.720000\n",
            "Epoch: 5/20... Step: 3700... Loss: 0.125471... Val Loss: 0.391443 Val Acc: 86.380000\n",
            "Epoch: 5/20... Step: 3800... Loss: 0.149574... Val Loss: 0.420004 Val Acc: 86.020000\n",
            "Epoch: 5/20... Step: 3900... Loss: 0.205040... Val Loss: 0.417260 Val Acc: 84.280000\n",
            "Epoch: 5/20... Step: 4000... Loss: 0.143841... Val Loss: 0.390816 Val Acc: 86.080000\n",
            "Epoch: 6/20... Step: 4100... Loss: 0.093473... Val Loss: 0.493684 Val Acc: 86.260000\n",
            "Epoch: 6/20... Step: 4200... Loss: 0.123091... Val Loss: 0.450911 Val Acc: 86.100000\n",
            "Epoch: 6/20... Step: 4300... Loss: 0.085430... Val Loss: 0.453689 Val Acc: 86.700000\n",
            "Model saved with higher validation accuracy: 86.700000\n",
            "Epoch: 6/20... Step: 4400... Loss: 0.027385... Val Loss: 0.484727 Val Acc: 85.360000\n",
            "Epoch: 6/20... Step: 4500... Loss: 0.012280... Val Loss: 0.472411 Val Acc: 86.280000\n",
            "Epoch: 6/20... Step: 4600... Loss: 0.082451... Val Loss: 0.446747 Val Acc: 86.060000\n",
            "Epoch: 6/20... Step: 4700... Loss: 0.082704... Val Loss: 0.488427 Val Acc: 85.620000\n",
            "Epoch: 6/20... Step: 4800... Loss: 0.014375... Val Loss: 0.447564 Val Acc: 85.840000\n",
            "Epoch: 7/20... Step: 4900... Loss: 0.008829... Val Loss: 0.591863 Val Acc: 86.240000\n",
            "Epoch: 7/20... Step: 5000... Loss: 0.008990... Val Loss: 0.580732 Val Acc: 85.340000\n",
            "Epoch: 7/20... Step: 5100... Loss: 0.044071... Val Loss: 0.605332 Val Acc: 85.180000\n",
            "Epoch: 7/20... Step: 5200... Loss: 0.012695... Val Loss: 0.519079 Val Acc: 85.360000\n",
            "Epoch: 7/20... Step: 5300... Loss: 0.156437... Val Loss: 0.543743 Val Acc: 85.660000\n",
            "Epoch: 7/20... Step: 5400... Loss: 0.030728... Val Loss: 0.540578 Val Acc: 85.100000\n",
            "Epoch: 7/20... Step: 5500... Loss: 0.022751... Val Loss: 0.578108 Val Acc: 85.760000\n",
            "Epoch: 7/20... Step: 5600... Loss: 0.195388... Val Loss: 0.527685 Val Acc: 85.500000\n",
            "Epoch: 8/20... Step: 5700... Loss: 0.019505... Val Loss: 0.611697 Val Acc: 85.820000\n",
            "Epoch: 8/20... Step: 5800... Loss: 0.002912... Val Loss: 0.591641 Val Acc: 85.680000\n",
            "Epoch: 8/20... Step: 5900... Loss: 0.046468... Val Loss: 0.595339 Val Acc: 85.120000\n",
            "Epoch: 8/20... Step: 6000... Loss: 0.042560... Val Loss: 0.641002 Val Acc: 84.860000\n",
            "Epoch: 8/20... Step: 6100... Loss: 0.001988... Val Loss: 0.658848 Val Acc: 84.460000\n",
            "Epoch: 8/20... Step: 6200... Loss: 0.062721... Val Loss: 0.649240 Val Acc: 84.860000\n",
            "Epoch: 8/20... Step: 6300... Loss: 0.003664... Val Loss: 0.650164 Val Acc: 85.520000\n",
            "Epoch: 8/20... Step: 6400... Loss: 0.047724... Val Loss: 0.607458 Val Acc: 85.220000\n",
            "Epoch: 9/20... Step: 6500... Loss: 0.017307... Val Loss: 0.782936 Val Acc: 84.840000\n",
            "Epoch: 9/20... Step: 6600... Loss: 0.001728... Val Loss: 0.710563 Val Acc: 85.200000\n",
            "Epoch: 9/20... Step: 6700... Loss: 0.004105... Val Loss: 0.695886 Val Acc: 85.320000\n",
            "Epoch: 9/20... Step: 6800... Loss: 0.018133... Val Loss: 0.756947 Val Acc: 83.860000\n",
            "Epoch: 9/20... Step: 6900... Loss: 0.071665... Val Loss: 0.630656 Val Acc: 85.420000\n",
            "Epoch: 9/20... Step: 7000... Loss: 0.017117... Val Loss: 0.629472 Val Acc: 86.080000\n",
            "Epoch: 9/20... Step: 7100... Loss: 0.013716... Val Loss: 0.723487 Val Acc: 84.200000\n",
            "Epoch: 9/20... Step: 7200... Loss: 0.019287... Val Loss: 0.794172 Val Acc: 84.780000\n",
            "Epoch: 10/20... Step: 7300... Loss: 0.140101... Val Loss: 0.915300 Val Acc: 83.460000\n",
            "Epoch: 10/20... Step: 7400... Loss: 0.001543... Val Loss: 0.719944 Val Acc: 85.260000\n",
            "Epoch: 10/20... Step: 7500... Loss: 0.060531... Val Loss: 0.792762 Val Acc: 84.680000\n",
            "Epoch: 10/20... Step: 7600... Loss: 0.000756... Val Loss: 0.787847 Val Acc: 85.520000\n",
            "Epoch: 10/20... Step: 7700... Loss: 0.023389... Val Loss: 0.764766 Val Acc: 84.340000\n",
            "Epoch: 10/20... Step: 7800... Loss: 0.019314... Val Loss: 0.748159 Val Acc: 84.460000\n",
            "Epoch: 10/20... Step: 7900... Loss: 0.093574... Val Loss: 0.645991 Val Acc: 85.280000\n",
            "Epoch: 10/20... Step: 8000... Loss: 0.015044... Val Loss: 0.691837 Val Acc: 84.680000\n",
            "Epoch: 11/20... Step: 8100... Loss: 0.001407... Val Loss: 0.858282 Val Acc: 83.820000\n",
            "Epoch: 11/20... Step: 8200... Loss: 0.002979... Val Loss: 0.699905 Val Acc: 84.860000\n",
            "Epoch: 11/20... Step: 8300... Loss: 0.004073... Val Loss: 0.769922 Val Acc: 84.860000\n",
            "Epoch: 11/20... Step: 8400... Loss: 0.042440... Val Loss: 0.786723 Val Acc: 84.520000\n",
            "Epoch: 11/20... Step: 8500... Loss: 0.096065... Val Loss: 0.714065 Val Acc: 85.780000\n",
            "Epoch: 11/20... Step: 8600... Loss: 0.090041... Val Loss: 0.839792 Val Acc: 85.320000\n",
            "Epoch: 11/20... Step: 8700... Loss: 0.015022... Val Loss: 0.745308 Val Acc: 84.900000\n",
            "Epoch: 11/20... Step: 8800... Loss: 0.051609... Val Loss: 0.758903 Val Acc: 85.660000\n",
            "Epoch: 12/20... Step: 8900... Loss: 0.022795... Val Loss: 0.807186 Val Acc: 84.900000\n",
            "Epoch: 12/20... Step: 9000... Loss: 0.001457... Val Loss: 0.828588 Val Acc: 83.960000\n",
            "Epoch: 12/20... Step: 9100... Loss: 0.004975... Val Loss: 0.729730 Val Acc: 85.180000\n",
            "Epoch: 12/20... Step: 9200... Loss: 0.000589... Val Loss: 0.815596 Val Acc: 85.340000\n",
            "Epoch: 12/20... Step: 9300... Loss: 0.002516... Val Loss: 0.791932 Val Acc: 85.620000\n",
            "Epoch: 12/20... Step: 9400... Loss: 0.008407... Val Loss: 0.751210 Val Acc: 85.020000\n",
            "Epoch: 12/20... Step: 9500... Loss: 0.053292... Val Loss: 0.657352 Val Acc: 84.560000\n",
            "Epoch: 12/20... Step: 9600... Loss: 0.005718... Val Loss: 0.739756 Val Acc: 85.500000\n",
            "Epoch: 13/20... Step: 9700... Loss: 0.004039... Val Loss: 0.776937 Val Acc: 84.280000\n",
            "Epoch: 13/20... Step: 9800... Loss: 0.002343... Val Loss: 0.916481 Val Acc: 84.480000\n",
            "Epoch: 13/20... Step: 9900... Loss: 0.000463... Val Loss: 0.950970 Val Acc: 84.940000\n",
            "Epoch: 13/20... Step: 10000... Loss: 0.040707... Val Loss: 0.813244 Val Acc: 85.420000\n",
            "Epoch: 13/20... Step: 10100... Loss: 0.072692... Val Loss: 0.829596 Val Acc: 85.380000\n",
            "Epoch: 13/20... Step: 10200... Loss: 0.112404... Val Loss: 0.819532 Val Acc: 85.800000\n",
            "Epoch: 13/20... Step: 10300... Loss: 0.011477... Val Loss: 0.798103 Val Acc: 85.480000\n",
            "Epoch: 13/20... Step: 10400... Loss: 0.062400... Val Loss: 0.851994 Val Acc: 84.700000\n",
            "Epoch: 14/20... Step: 10500... Loss: 0.000403... Val Loss: 0.897334 Val Acc: 85.200000\n",
            "Epoch: 14/20... Step: 10600... Loss: 0.000298... Val Loss: 0.948146 Val Acc: 84.440000\n",
            "Epoch: 14/20... Step: 10700... Loss: 0.001593... Val Loss: 0.923257 Val Acc: 84.980000\n",
            "Epoch: 14/20... Step: 10800... Loss: 0.000469... Val Loss: 0.913430 Val Acc: 84.200000\n",
            "Epoch: 14/20... Step: 10900... Loss: 0.019585... Val Loss: 0.927489 Val Acc: 84.840000\n",
            "Epoch: 14/20... Step: 11000... Loss: 0.000556... Val Loss: 0.847836 Val Acc: 84.640000\n",
            "Epoch: 14/20... Step: 11100... Loss: 0.015845... Val Loss: 0.906371 Val Acc: 84.680000\n",
            "Epoch: 14/20... Step: 11200... Loss: 0.006793... Val Loss: 1.003053 Val Acc: 83.220000\n",
            "Epoch: 15/20... Step: 11300... Loss: 0.000740... Val Loss: 0.819256 Val Acc: 84.960000\n",
            "Epoch: 15/20... Step: 11400... Loss: 0.023220... Val Loss: 0.810002 Val Acc: 85.120000\n",
            "Epoch: 15/20... Step: 11500... Loss: 0.004809... Val Loss: 0.892121 Val Acc: 85.300000\n",
            "Epoch: 15/20... Step: 11600... Loss: 0.000796... Val Loss: 0.900915 Val Acc: 85.300000\n",
            "Epoch: 15/20... Step: 11700... Loss: 0.001318... Val Loss: 0.878319 Val Acc: 85.960000\n",
            "Epoch: 15/20... Step: 11800... Loss: 0.003796... Val Loss: 0.881795 Val Acc: 84.360000\n",
            "Epoch: 15/20... Step: 11900... Loss: 0.000406... Val Loss: 0.888327 Val Acc: 85.120000\n",
            "Epoch: 15/20... Step: 12000... Loss: 0.001247... Val Loss: 0.952150 Val Acc: 84.820000\n",
            "Epoch: 16/20... Step: 12100... Loss: 0.119340... Val Loss: 0.981834 Val Acc: 84.460000\n",
            "Epoch: 16/20... Step: 12200... Loss: 0.000699... Val Loss: 0.905985 Val Acc: 85.540000\n",
            "Epoch: 16/20... Step: 12300... Loss: 0.001013... Val Loss: 0.846589 Val Acc: 84.400000\n",
            "Epoch: 16/20... Step: 12400... Loss: 0.002544... Val Loss: 0.860423 Val Acc: 84.480000\n",
            "Epoch: 16/20... Step: 12500... Loss: 0.000648... Val Loss: 0.869743 Val Acc: 84.800000\n",
            "Epoch: 16/20... Step: 12600... Loss: 0.017709... Val Loss: 0.879568 Val Acc: 85.440000\n",
            "Epoch: 16/20... Step: 12700... Loss: 0.089023... Val Loss: 0.849969 Val Acc: 85.020000\n",
            "Epoch: 16/20... Step: 12800... Loss: 0.036977... Val Loss: 0.827590 Val Acc: 85.520000\n",
            "Epoch: 17/20... Step: 12900... Loss: 0.000220... Val Loss: 0.916259 Val Acc: 85.560000\n",
            "Epoch: 17/20... Step: 13000... Loss: 0.000596... Val Loss: 0.817464 Val Acc: 85.140000\n",
            "Epoch: 17/20... Step: 13100... Loss: 0.007187... Val Loss: 0.934669 Val Acc: 84.840000\n",
            "Epoch: 17/20... Step: 13200... Loss: 0.000313... Val Loss: 0.926657 Val Acc: 84.600000\n",
            "Epoch: 17/20... Step: 13300... Loss: 0.007543... Val Loss: 0.822136 Val Acc: 85.020000\n",
            "Epoch: 17/20... Step: 13400... Loss: 0.000626... Val Loss: 0.941131 Val Acc: 85.440000\n",
            "Epoch: 17/20... Step: 13500... Loss: 0.000340... Val Loss: 0.896315 Val Acc: 84.900000\n",
            "Epoch: 17/20... Step: 13600... Loss: 0.000209... Val Loss: 0.904401 Val Acc: 85.380000\n",
            "Epoch: 18/20... Step: 13700... Loss: 0.000148... Val Loss: 0.930290 Val Acc: 84.980000\n",
            "Epoch: 18/20... Step: 13800... Loss: 0.000577... Val Loss: 0.962599 Val Acc: 84.480000\n",
            "Epoch: 18/20... Step: 13900... Loss: 0.008964... Val Loss: 0.954999 Val Acc: 82.800000\n",
            "Epoch: 18/20... Step: 14000... Loss: 0.069130... Val Loss: 0.877921 Val Acc: 84.860000\n",
            "Epoch: 18/20... Step: 14100... Loss: 0.000822... Val Loss: 0.950859 Val Acc: 84.220000\n",
            "Epoch: 18/20... Step: 14200... Loss: 0.001087... Val Loss: 1.004312 Val Acc: 82.460000\n",
            "Epoch: 18/20... Step: 14300... Loss: 0.000413... Val Loss: 0.856460 Val Acc: 85.380000\n",
            "Epoch: 18/20... Step: 14400... Loss: 0.001545... Val Loss: 0.889437 Val Acc: 85.380000\n",
            "Epoch: 19/20... Step: 14500... Loss: 0.000470... Val Loss: 0.924884 Val Acc: 84.880000\n",
            "Epoch: 19/20... Step: 14600... Loss: 0.003093... Val Loss: 0.918201 Val Acc: 85.100000\n",
            "Epoch: 19/20... Step: 14700... Loss: 0.000313... Val Loss: 0.845988 Val Acc: 84.320000\n",
            "Epoch: 19/20... Step: 14800... Loss: 0.003129... Val Loss: 0.945009 Val Acc: 83.820000\n",
            "Epoch: 19/20... Step: 14900... Loss: 0.000098... Val Loss: 0.990425 Val Acc: 85.040000\n",
            "Epoch: 19/20... Step: 15000... Loss: 0.000653... Val Loss: 0.925440 Val Acc: 84.620000\n",
            "Epoch: 19/20... Step: 15100... Loss: 0.000189... Val Loss: 0.863074 Val Acc: 85.080000\n",
            "Epoch: 19/20... Step: 15200... Loss: 0.000866... Val Loss: 0.906017 Val Acc: 84.460000\n",
            "Epoch: 20/20... Step: 15300... Loss: 0.000261... Val Loss: 0.910748 Val Acc: 84.480000\n",
            "Epoch: 20/20... Step: 15400... Loss: 0.000597... Val Loss: 0.945448 Val Acc: 84.480000\n",
            "Epoch: 20/20... Step: 15500... Loss: 0.000079... Val Loss: 0.950606 Val Acc: 84.860000\n",
            "Epoch: 20/20... Step: 15600... Loss: 0.026513... Val Loss: 1.029075 Val Acc: 84.740000\n",
            "Epoch: 20/20... Step: 15700... Loss: 0.008002... Val Loss: 1.085763 Val Acc: 84.440000\n",
            "Epoch: 20/20... Step: 15800... Loss: 0.000059... Val Loss: 1.133464 Val Acc: 84.620000\n",
            "Epoch: 20/20... Step: 15900... Loss: 0.003445... Val Loss: 1.028817 Val Acc: 84.520000\n",
            "Epoch: 20/20... Step: 16000... Loss: 0.000463... Val Loss: 0.971691 Val Acc: 84.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score"
      ],
      "metadata": {
        "id": "D4Evs9CaeJuv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net_2 = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net_2)\n",
        "\n",
        "\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net_2.parameters(), lr=lr)\n",
        "\n",
        "# Load the saved model state\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "net_2.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xTwamhx0Yn5",
        "outputId": "2a2b63ee-563b-4b95-ea39-a521e3a8c6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(181686, 512)\n",
            "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "test_predictions = []\n",
        "h = net_2.init_hidden(batch_size)\n",
        "f1_scores = []\n",
        "recalls = []\n",
        "precisions = []\n",
        "\n",
        "net_2.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "  # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        net_2.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net_2(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    ## compute the f1 score\n",
        "    f1_scores.append(f1_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    recalls.append(recall_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    precisions.append(precision_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    ## compute the recall\n",
        "    ## compute precision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "print(\"F1 score:\",np.mean(f1_scores))\n",
        "print(\"Recall:\",np.mean(recalls))\n",
        "print(\"precisions:\",np.mean(precisions))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "id": "A8YCHQtzdQJy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "944189dc-2a76-4a2d-eb0b-895321e44c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.468\n",
            "F1 score: 0.8600469292239535\n",
            "Recall: 0.8699571805623154\n",
            "precisions: 0.8549720679413203\n",
            "Test accuracy: 0.863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1,161)\n",
        "actual_e = epochs/8"
      ],
      "metadata": {
        "id": "G1r6n7pzDlBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(actual_e, np.array(epoch_losses))\n",
        "plt.title('Epoch vs Validation accuracy')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "A9Eq_YKLDSFy",
        "outputId": "da10780c-d7a5-4038-c2de-d88f42c1b8dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kq5cNWoIGdqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_review(test_review):\n",
        "    test_review = test_review.lower() # lowercase\n",
        "    # get rid of punctuation\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "\n",
        "    # splitting by spaces\n",
        "    test_words = test_text.split()\n",
        "\n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([word_to_index_synthetic.get(word, 0) for word in test_words])\n",
        "\n",
        "    return test_ints\n"
      ],
      "metadata": {
        "id": "sr9P0yS8vncL"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(net, test_review, sequence_length=200):\n",
        "\n",
        "    net.eval()\n",
        "\n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "\n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "\n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "\n",
        "    batch_size = feature_tensor.size(0)\n",
        "\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "\n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "\n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Positive review detected!\")\n",
        "    else:\n",
        "        print(\"Negative review detected.\")"
      ],
      "metadata": {
        "id": "e5lw_4E7v7JI"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_review_pos = 'The Hunger Games: The Ballad Of Songbirds & Snakes is the perfect masterpiece haunting origin tale showing the downfall of president snow. What a masterful job at taking the book and adapting it to the screen. The cinematography is excellent.'"
      ],
      "metadata": {
        "id": "yGYEPoFiwC3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net_2, test_review_pos, seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf2kKSypwIzb",
        "outputId": "f69643a2-d02a-4e45-aa8d-72c11415fc64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.999517\n",
            "Positive review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_review='Good movie'"
      ],
      "metadata": {
        "id": "7yD8vH52wMjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length=200\n",
        "\n",
        "predict(net_2, my_review, seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uUgGzol0xDU",
        "outputId": "0fe88d59-b09e-4cc5-e3fc-877cbe9fb8d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.993136\n",
            "Positive review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net_2, \"I enjoyed the movie!\", seq_length)"
      ],
      "metadata": {
        "id": "FrHkgeWl00PY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ca9041-94d4-424c-a129-0de4f4b06816"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.970758\n",
            "Positive review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net_2, \"I hated the acting and the plot!\", seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpADlbOwMYrf",
        "outputId": "4b87dffa-3822-4396-b2ce-86b71c3cb7b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.082474\n",
            "Negative review detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net_2, \"i did not love the vibrant colors in the movie\", seq_length)"
      ],
      "metadata": {
        "id": "IHjBIkErMbVv",
        "outputId": "f7d872c0-7d6e-442d-acda-185d6f273c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.064976\n",
            "Negative review detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training with Synthetic dataset"
      ],
      "metadata": {
        "id": "AFD9-o99qhcr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_synthetic=pd.read_csv(\"/content/googledrive/MyDrive/NLP Project/synthetic_data.csv\")\n",
        "df_synthetic.head(10)"
      ],
      "metadata": {
        "id": "GPJR6aHU-6I5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "5d6109b2-aa62-4130-b2ef-26ce74cb4b17"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  label\n",
              "0   hickok see perfect overhead man pretentious r...      1\n",
              "1   high film idea problem even bush porn helping...      0\n",
              "2   grave one supposed message make audience mixe...      0\n",
              "3   another clear novelist film solitude movie wi...      1\n",
              "4   book standard smart energy drowning show mess...      1\n",
              "5   offend actually acting bloody future narrowly...      1\n",
              "6   exploit niro polled coyote costume favorite k...      1\n",
              "7   almost good must movie one still jerk since i...      0\n",
              "8   mystery see movie exit lovable ironic monkey ...      0\n",
              "9   using film friend system would film photoserv...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2d9a7d4c-fa5f-4bea-8902-f93d694e4051\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hickok see perfect overhead man pretentious r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>high film idea problem even bush porn helping...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>grave one supposed message make audience mixe...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>another clear novelist film solitude movie wi...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>book standard smart energy drowning show mess...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>offend actually acting bloody future narrowly...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>exploit niro polled coyote costume favorite k...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>almost good must movie one still jerk since i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>mystery see movie exit lovable ironic monkey ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>using film friend system would film photoserv...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2d9a7d4c-fa5f-4bea-8902-f93d694e4051')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2d9a7d4c-fa5f-4bea-8902-f93d694e4051 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2d9a7d4c-fa5f-4bea-8902-f93d694e4051');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d4e32bab-8764-488c-94a4-e8029203b86d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d4e32bab-8764-488c-94a4-e8029203b86d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d4e32bab-8764-488c-94a4-e8029203b86d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Shape of synthetic dataset\", df_synthetic.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sob14cXDq8KH",
        "outputId": "ffd28008-0d96-46b9-aa4f-58261d91598b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of synthetic dataset (1000, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(punctuation)\n",
        "\n",
        "# Get rid of punctuation\n",
        "df_synthetic['review'] = df_synthetic['review'].str.lower()\n",
        "df_synthetic['review'] = df_synthetic['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
        "\n",
        "# Split by new lines and spaces\n",
        "df_synthetic['review'] = df_synthetic['review'].str.replace('\\n', ' ')\n",
        "df_synthetic['review'] =df_synthetic['review'].str.split()\n",
        "\n",
        "# Create a list of words\n",
        "df_synthetic['review'] = df_synthetic['review'].apply(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "BVO7Zps_-6Ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46f98ea9-3faa-4039-a958-5308fe178346"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(df_column):\n",
        "\n",
        "\n",
        "    reviews = df_column.values\n",
        "\n",
        "    # Tokenize the reviews\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(reviews)\n",
        "    sequences = tokenizer.texts_to_sequences(reviews)\n",
        "\n",
        "    # Get the dictionary\n",
        "    word_to_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "    print(\"Dictionary: \", dict(list(word_to_index.items())[:5]))\n",
        "\n",
        "    return tokenizer, sequences, word_to_index"
      ],
      "metadata": {
        "id": "grxxOHhBrgjt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_synthetic, sequences_synthetic, word_to_index_synthetic = tokenize(df_synthetic['review'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUjjhk8isNgt",
        "outputId": "1d1445bb-ade9-412a-e5b9-39373748506f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary:  {'movie': 1, 'film': 2, 'one': 3, 'like': 4, 'good': 5}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_split_synthetic = df_synthetic['review'].tolist()\n",
        "labels_split_synthetic= df_synthetic['label'].tolist()"
      ],
      "metadata": {
        "id": "-0aaRfausXsF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_labels=np.array(labels_split_synthetic)"
      ],
      "metadata": {
        "id": "fEomdzozuurx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_ints_synthetic = tokenize_reviews(reviews_split_synthetic, word_to_index_synthetic)"
      ],
      "metadata": {
        "id": "l3KCsiXttAFc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "seq_length = 200\n",
        "\n",
        "features_synthetic = pad_features(reviews_ints_synthetic, seq_length=seq_length)\n",
        "\n",
        "\n",
        "assert len(features_synthetic)==len(reviews_ints_synthetic), \"Your features should have as many rows as reviews.\"\n",
        "assert len(features_synthetic[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
        "\n",
        "\n",
        "print(features_synthetic[:30,:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE9D6tePtanB",
        "outputId": "8bd012c7-2437-45d9-9038-a73ec5ec8e88"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[7977   12  314 4064   51 1240 1615 1403  389  958]\n",
            " [ 227    2  153  173    9 2824 1076 1779    3  611]\n",
            " [2450    3  273  463   10  177 1781 4070  411  770]\n",
            " [  63  563 4074    2 5292    1 4075   83    1 7998]\n",
            " [ 148  822  857 1498 4083   24  637  206 8003 8004]\n",
            " [5296   70   45 1619  736 8007    3 8008 8009   11]\n",
            " [4091 5301 8017 5302  614  315  137  281   68   80]\n",
            " [ 133    5   91    1    3   53 2833  136  362 1785]\n",
            " [ 505   12    1 4095 1788 2458 1079   10    2   71]\n",
            " [ 775    2  112 2459    8    2 8034 8035  117   20]\n",
            " [2190  411   24 3349 8038 1965 8039  369 8040 2196]\n",
            " [1330  800  159  105 8051  190 8052 3352  174 2203]\n",
            " [4106 1628 1248 1333 4107 1413 8061  329  212 5313]\n",
            " [ 206  663  281   33 8076   36 5316 1972 1794   32]\n",
            " [   6 8083  112 1504   54 1161   42 4114  301   20]\n",
            " [  15 4118  866   33 4119    3 3357 4120   17    2]\n",
            " [ 259  190 5326  150  160 4123  740 1164  711    1]\n",
            " [5329 8105 8106  216  306  306   43 8107 8108  365]\n",
            " [1339  708 5345 1257   26 8129 1258  206   71  280]\n",
            " [  34  127    4 8142 1984 1985  618 1643  665   45]\n",
            " [ 118 1515  265 8157    2  968 1801 1020  248  508]\n",
            " [ 617 1804  273   72   22 1013   41  828   73  279]\n",
            " [  16  253 1090  129 8187 3378    6  202 1806   72]\n",
            " [5372 1016 4144 1020  668 1242 1995   70  972   27]\n",
            " [5381  157 8205    2   35 1267 8206    1  312  152]\n",
            " [5387   18 5388  339  538 4150   26 8216  286 8217]\n",
            " [3339  688  146 1406 8223 1652  549  344 8224  929]\n",
            " [  87 1349   33    3 2887 1180  180   45 4163  145]\n",
            " [ 546  243   20 2890   22 8240   16  362 8241 5397]\n",
            " [  14  831 1095    2 2894 1818   47 8257  182   20]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "# Split data into training, validation, and test data\n",
        "train_idx = int(len(features_synthetic) * split_frac)\n",
        "train_x, remaining_x = features_synthetic[:train_idx], features_synthetic[train_idx:]\n",
        "train_y, remaining_y = encoded_labels[:train_idx], encoded_labels[train_idx:]\n",
        "\n",
        "test_val_idx = int(len(remaining_x) * 0.5)\n",
        "val_idx = test_val_idx\n",
        "test_idx = test_val_idx + int(len(remaining_x) * 0.5)\n",
        "\n",
        "val_x, test_x = remaining_x[:val_idx], remaining_x[val_idx:test_idx]\n",
        "val_y, test_y = remaining_y[:val_idx], remaining_y[val_idx:test_idx]\n",
        "\n",
        "\n",
        "print(\"\\t\\t\\tFeature Shapes:\")\n",
        "print(\"Train set: \\t\\t{}\".format(train_x.shape),\n",
        "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
        "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhH7l4hptyRp",
        "outputId": "a79b8d8c-2736-4bfe-a918-13a0ecb3aaf7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t\t\tFeature Shapes:\n",
            "Train set: \t\t(800, 200) \n",
            "Validation set: \t(100, 200) \n",
            "Test set: \t\t(100, 200)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size,  drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
      ],
      "metadata": {
        "id": "MhAWcUO4uP5T"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index_synthetic)+1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 512\n",
        "n_layers = 2\n",
        "\n",
        "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "\n",
        "print(net)\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "checkpoint = torch.load('/content/googledrive/MyDrive/NLP Project/best_model_synthetic.pth', map_location=torch.device('cpu'))\n",
        "net.load_state_dict(checkpoint['state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTe6Nt49uYRh",
        "outputId": "b80f5616-77d5-4f49-8ab5-da884f3952c5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SentimentLSTM(\n",
            "  (embedding): Embedding(17796, 512)\n",
            "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_losses = []\n",
        "num_correct = 0\n",
        "test_predictions = []\n",
        "h = net.init_hidden(batch_size)\n",
        "f1_scores = []\n",
        "recalls = []\n",
        "precisions = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "  # Creating new variables for the hidden state\n",
        "    h = tuple([each.data for each in h])\n",
        "\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        net.cuda()\n",
        "\n",
        "    # get predicted outputs\n",
        "    output, h = net(inputs, h)\n",
        "\n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "    ## compute the f1 score\n",
        "    f1_scores.append(f1_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    recalls.append(recall_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    precisions.append(precision_score(labels.cpu().detach().numpy(),pred.cpu().detach().numpy()))\n",
        "    ## compute the recall\n",
        "    ## compute precision\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "print(\"F1 score:\",np.mean(f1_scores))\n",
        "print(\"Recall:\",np.mean(recalls))\n",
        "print(\"precisions:\",np.mean(precisions))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoWm2MwccJ3_",
        "outputId": "27506a75-f27c-4180-b33b-2c3a6372b7d8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.747\n",
            "F1 score: 0.4657849338700403\n",
            "Recall: 0.37962962962962965\n",
            "precisions: 0.6595238095238095\n",
            "Test accuracy: 0.560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net, \"I enjoyed the movie!\", seq_length)"
      ],
      "metadata": {
        "id": "TmIaePypwZxq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d883ded1-a2cc-4d68-a405-c446062e389c"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.505942\n",
            "Positive review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net, \"Horrible movie\", seq_length)"
      ],
      "metadata": {
        "id": "B6mBYbP1Kt1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa401c6c-a332-4136-cec2-1318c6080746"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.497262\n",
            "Negative review detected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# call function\n",
        "seq_length=200\n",
        "\n",
        "predict(net, \"i did not love the vibrant colors in the movie\", seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NRyKNlnbREr",
        "outputId": "37a4d169-4c81-4776-9368-3309ed764631"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction value, pre-rounding: 0.509269\n",
            "Positive review detected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EKEliJpYbXq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}